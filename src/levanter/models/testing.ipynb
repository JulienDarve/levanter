{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-01 14:42:02,038\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import roberta as my_roberta\n",
    "from transformers.models.roberta import modeling_roberta as hf_roberta\n",
    "\n",
    "import torch\n",
    "import haliax as hax\n",
    "import jax.random as jrandom\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "hf_model_str = \"FacebookAI/roberta-base\"\n",
    "\n",
    "hf_config = AutoConfig.from_pretrained(hf_model_str)\n",
    "hf_config.hidden_dropout_prob = 0\n",
    "hf_config.attention_probs_dropout_prob = 0\n",
    "my_config = my_roberta.RobertaConfig.from_hf_config(hf_config)\n",
    "\n",
    "key = jrandom.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmbedAtt = my_config.EmbedAtt\n",
    "Embed = my_config.Embed\n",
    "Mlp = my_config.Mlp\n",
    "Pos = my_config.Pos\n",
    "KeyPos = my_config.KeyPos\n",
    "Batch = hax.Axis(\"batch\", 2)\n",
    "\n",
    "x_embed = hax.random.normal(key, (Batch, Pos, Embed))\n",
    "x_embed_att = hax.random.normal(key, (Batch, Pos, EmbedAtt))\n",
    "x_mlp = hax.random.normal(key, (Batch, Pos, Mlp))\n",
    "# x = x[{\"position\": slice(Pos.size-2)}]\n",
    "\n",
    "x_embed_torch = torch.from_numpy(np.array(x_embed.array))\n",
    "x_embed_att_torch = torch.from_numpy(np.array(x_embed_att.array))\n",
    "x_mlp_torch = torch.from_numpy(np.array(x_mlp.array))\n",
    "\n",
    "mask = hax.ones((Batch, Pos))[{\"position\": slice(0,-2)}]\n",
    "mask_torch = torch.from_numpy(np.array(mask.array))\n",
    "# mask_torch = torch.ones((2, hf_config.num_attention_heads, hf_config.max_position_embeddings -2, hf_config.max_position_embeddings -2))\n",
    "\n",
    "Vocab = hax.Axis(\"vocab\", my_config.vocab_size)\n",
    "\n",
    "input_embeds = hax.random.uniform(key, (Batch, Pos, Embed))\n",
    "input_embeds = input_embeds[{\"position\": slice(0,-2)}]\n",
    "\n",
    "input_ids = hax.random.randint(key, (Batch, Pos), minval = 0, maxval = my_config.vocab_size)\n",
    "input_ids = input_ids[{\"position\": slice(0,-2)}]\n",
    "\n",
    "input_embeds_torch = torch.from_numpy(np.array(input_embeds.array))\n",
    "input_ids_torch = torch.from_numpy(np.array(input_ids.array))\n",
    "\n",
    "features = hax.random.normal(key, (Batch, Embed))\n",
    "features_torch = torch.from_numpy(np.array(features.array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(hf_model_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(my_output, hf_output, p=False, pp=False, ppp=False, pppp=True, precision=1e-4):\n",
    "    \n",
    "    assert (np.array(my_output.shape) == np.array(hf_output.shape)).all()\n",
    "    # print(my_output.shape)\n",
    "    # print(hf_output.shape)\n",
    "\n",
    "    success = np.isclose(hf_output, np.array(my_output), rtol=precision, atol=precision).all()\n",
    "\n",
    "    if success:\n",
    "        print(\"Success!!!\")\n",
    "    else:\n",
    "        print(\"Fail :((((\")\n",
    "    \n",
    "    if ppp:\n",
    "        acc_prev = None\n",
    "        for i in range(15):\n",
    "            prec = 10 ** (-1*i)\n",
    "            acc = np.isclose(hf_output, np.array(my_output), rtol=precision, atol=prec).mean()\n",
    "            if acc_prev is None:\n",
    "                print(f\"Iteration {i}, Precision {prec}:\\t{acc}\")\n",
    "            else:\n",
    "                if np.abs(acc - acc_prev) > 1e-4:\n",
    "                    print(f\"Iteration {i}, Precision {prec}:\\t{acc}\")\n",
    "            acc_prev = acc\n",
    "        print(f\"Iteration {i}, Precision {prec}:\\t{acc}\")\n",
    "\n",
    "    if pppp:\n",
    "        acc = np.isclose(hf_output, np.array(my_output), rtol=precision, atol=precision).mean()\n",
    "        print(f\"Accuracy: {acc}\")\n",
    "    \n",
    "    if p:   \n",
    "        acc = np.isclose(hf_output, np.array(my_output), rtol=precision, atol=precision).mean()\n",
    "        print(f\"Accuracy: {acc}\")\n",
    "        print(f\"Jax:\\n{torch.tensor(np.array(my_output))}\\nTorch:\\n{hf_output}\")\n",
    "\n",
    "    if pp:\n",
    "        diff = torch.tensor(np.array(my_output)) - hf_output\n",
    "        print(f\"Mean: {diff.abs().mean()}\")\n",
    "        print(f\"Stdev: {diff.std()}\")\n",
    "        print(f\"Difference:\\n{diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaSelfOutput\\n\\nmy_self_output = my_roberta.RobertaSelfOutput.init(my_config, key=key)\\nstate = my_self_output.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_self_output = hf_roberta.RobertaSelfOutput(hf_config)\\nhf_self_output.load_state_dict(state, strict=True)\\n\\nmy_output = my_self_output(x_embed_att, x_embed, key=key)\\nhf_output = hf_self_output(x_embed_att_torch, x_embed_torch)\\n\\ncheck(my_output.array, hf_output.detach(), ppp=True)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaSelfOutput\n",
    "\n",
    "my_self_output = my_roberta.RobertaSelfOutput.init(my_config, key=key)\n",
    "state = my_self_output.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_self_output = hf_roberta.RobertaSelfOutput(hf_config)\n",
    "hf_self_output.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_self_output(x_embed_att, x_embed, key=key)\n",
    "hf_output = hf_self_output(x_embed_att_torch, x_embed_torch)\n",
    "\n",
    "check(my_output.array, hf_output.detach(), ppp=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaSelfAttention\\n\\nmy_attn_output = my_roberta.RobertaSelfAttention.init(my_config, key=key)\\nstate = my_attn_output.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_attn_output  = hf_roberta.RobertaSelfAttention(hf_config)\\nhf_attn_output.load_state_dict(state, strict=True)\\n\\nmy_output = my_attn_output(x_embed, mask, key=key)\\nhf_output = hf_attn_output(x_embed_torch, mask_torch)\\n\\ncheck(my_output.array, hf_output[0].detach(), ppp=True)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaSelfAttention\n",
    "\n",
    "my_attn_output = my_roberta.RobertaSelfAttention.init(my_config, key=key)\n",
    "state = my_attn_output.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_attn_output  = hf_roberta.RobertaSelfAttention(hf_config)\n",
    "hf_attn_output.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_attn_output(x_embed, mask, key=key)\n",
    "hf_output = hf_attn_output(x_embed_torch, mask_torch)\n",
    "\n",
    "check(my_output.array, hf_output[0].detach(), ppp=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaAttention\\n\\nmy_func = my_roberta.RobertaAttention.init(my_config, key=key)\\nstate = my_func.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_func = hf_roberta.RobertaAttention(hf_config)\\nhf_func.load_state_dict(state, strict=True)\\n\\nmy_output = my_func(hidden_states=x_embed, attention_mask=mask, key=key)\\nhf_output = hf_func(hidden_states=x_embed_torch, attention_mask=mask_torch)\\n\\ncheck(my_output.array, hf_output[0].detach(), ppp=True)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaAttention\n",
    "\n",
    "my_func = my_roberta.RobertaAttention.init(my_config, key=key)\n",
    "state = my_func.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_func = hf_roberta.RobertaAttention(hf_config)\n",
    "hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_func(hidden_states=x_embed, attention_mask=mask, key=key)\n",
    "hf_output = hf_func(hidden_states=x_embed_torch, attention_mask=mask_torch)\n",
    "\n",
    "check(my_output.array, hf_output[0].detach(), ppp=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaIntermediate\\n\\nmy_int_output = my_roberta.RobertaIntermediate.init(my_config, key=key)\\nstate = my_int_output.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_int_output = hf_roberta.RobertaIntermediate(hf_config)\\nhf_int_output.load_state_dict(state, strict=True)\\n\\nmy_output = my_int_output(x_embed, key=key)\\nhf_output = hf_int_output(x_embed_torch)\\n\\ncheck(my_output.array, hf_output.detach(), ppp=True)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaIntermediate\n",
    "\n",
    "my_int_output = my_roberta.RobertaIntermediate.init(my_config, key=key)\n",
    "state = my_int_output.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_int_output = hf_roberta.RobertaIntermediate(hf_config)\n",
    "hf_int_output.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_int_output(x_embed, key=key)\n",
    "hf_output = hf_int_output(x_embed_torch)\n",
    "\n",
    "check(my_output.array, hf_output.detach(), ppp=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaOutput\\n\\nmy_int_output = my_roberta.RobertaOutput.init(my_config, key=key)\\nstate = my_int_output.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_int_output = hf_roberta.RobertaOutput(hf_config)\\nhf_int_output.load_state_dict(state, strict=True)\\n\\nmy_output = my_int_output(x_mlp, x_embed, key=key)\\nhf_output = hf_int_output(x_mlp_torch, x_embed_torch)\\n\\ncheck(my_output.array, hf_output.detach(), ppp=True)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaOutput\n",
    "\n",
    "my_int_output = my_roberta.RobertaOutput.init(my_config, key=key)\n",
    "state = my_int_output.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_int_output = hf_roberta.RobertaOutput(hf_config)\n",
    "hf_int_output.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_int_output(x_mlp, x_embed, key=key)\n",
    "hf_output = hf_int_output(x_mlp_torch, x_embed_torch)\n",
    "\n",
    "check(my_output.array, hf_output.detach(), ppp=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaLayer\\n\\nmy_func = my_roberta.RobertaLayer.init(my_config, key=key)\\nstate = my_func.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_func = hf_roberta.RobertaLayer(hf_config)\\nhf_func.load_state_dict(state, strict=True)\\n\\nmy_output = my_func(hidden_states=x_embed, attention_mask=mask, key=key)\\nhf_output = hf_func(hidden_states=x_embed_torch, attention_mask=mask_torch)\\n\\ncheck(my_output.array, hf_output[0].detach(), False, False, True, 1e-4)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaLayer\n",
    "\n",
    "my_func = my_roberta.RobertaLayer.init(my_config, key=key)\n",
    "state = my_func.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_func = hf_roberta.RobertaLayer(hf_config)\n",
    "hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_func(hidden_states=x_embed, attention_mask=mask, key=key)\n",
    "hf_output = hf_func(hidden_states=x_embed_torch, attention_mask=mask_torch)\n",
    "\n",
    "check(my_output.array, hf_output[0].detach(), False, False, True, 1e-4)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaEncoder\\n\\nmy_func = my_roberta.RobertaEncoder.init(my_config, key=key)\\nstate = my_func.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_func = hf_roberta.RobertaEncoder(hf_config)\\nhf_func.load_state_dict(state, strict=True)\\n\\nmy_output = my_func(hidden_states=x_embed, attention_mask=mask, key=key)\\nhf_output = hf_func(hidden_states=x_embed_torch, attention_mask=mask_torch)\\n\\ncheck(my_output.array, hf_output[0].detach(), False, False, True, 1e-4)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaEncoder\n",
    "\n",
    "my_func = my_roberta.RobertaEncoder.init(my_config, key=key)\n",
    "state = my_func.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_func = hf_roberta.RobertaEncoder(hf_config)\n",
    "hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_func(hidden_states=x_embed, attention_mask=mask, key=key)\n",
    "hf_output = hf_func(hidden_states=x_embed_torch, attention_mask=mask_torch)\n",
    "\n",
    "check(my_output.array, hf_output[0].detach(), False, False, True, 1e-4)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaEmbedding\\n\\nVocab = hax.Axis(\"vocab\", my_config.vocab_size)\\n\\ninput_embeds = hax.random.uniform(key, (Batch, Pos, Embed))\\ninput_embeds = input_embeds[{\"position\": slice(0,-2)}]\\ninput_ids = hax.random.randint(key, (Batch, Pos), minval = 0, maxval = my_config.vocab_size)\\ninput_ids = input_ids[{\"position\": slice(0,-2)}]\\n\\ninput_embeds_torch = torch.from_numpy(np.array(input_embeds.array))\\ninput_ids_torch = torch.from_numpy(np.array(input_ids.array))\\n\\nmy_func = my_roberta.RobertaEmbedding.init(Vocab, my_config, key=key)\\nstate = my_func.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_func = hf_roberta.RobertaEmbeddings(hf_config)\\nhf_func.load_state_dict(state, strict=True)\\n\\n# my_output = my_func.embed(input_embeds=input_embeds, key=key)\\n# hf_output = hf_func(inputs_embeds=input_embeds_torch)\\n\\nmy_output = my_func.embed(input_ids=input_ids, key=key)\\nhf_output = hf_func(input_ids=input_ids_torch)\\n\\ncheck(my_output.array, hf_output.detach())'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaEmbedding\n",
    "\n",
    "Vocab = hax.Axis(\"vocab\", my_config.vocab_size)\n",
    "\n",
    "input_embeds = hax.random.uniform(key, (Batch, Pos, Embed))\n",
    "input_embeds = input_embeds[{\"position\": slice(0,-2)}]\n",
    "input_ids = hax.random.randint(key, (Batch, Pos), minval = 0, maxval = my_config.vocab_size)\n",
    "input_ids = input_ids[{\"position\": slice(0,-2)}]\n",
    "\n",
    "input_embeds_torch = torch.from_numpy(np.array(input_embeds.array))\n",
    "input_ids_torch = torch.from_numpy(np.array(input_ids.array))\n",
    "\n",
    "my_func = my_roberta.RobertaEmbedding.init(Vocab, my_config, key=key)\n",
    "state = my_func.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_func = hf_roberta.RobertaEmbeddings(hf_config)\n",
    "hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "# my_output = my_func.embed(input_embeds=input_embeds, key=key)\n",
    "# hf_output = hf_func(inputs_embeds=input_embeds_torch)\n",
    "\n",
    "my_output = my_func.embed(input_ids=input_ids, key=key)\n",
    "hf_output = hf_func(input_ids=input_ids_torch)\n",
    "\n",
    "check(my_output.array, hf_output.detach())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaPooler\\n\\nmy_pool = my_roberta.RobertaPooler.init(my_config, key=key)\\nstate = my_pool.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_pool = hf_roberta.RobertaPooler(hf_config)\\nhf_pool.load_state_dict(state, strict=True)\\n\\nmy_output = my_pool(x_embed, key=key)\\nhf_output = hf_pool(x_embed_torch)\\n\\ncheck(my_output.array, hf_output.detach(), ppp=True)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaPooler\n",
    "\n",
    "my_pool = my_roberta.RobertaPooler.init(my_config, key=key)\n",
    "state = my_pool.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_pool = hf_roberta.RobertaPooler(hf_config)\n",
    "hf_pool.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_pool(x_embed, key=key)\n",
    "hf_output = hf_pool(x_embed_torch)\n",
    "\n",
    "check(my_output.array, hf_output.detach(), ppp=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaModel\\n\\nmy_pool = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=True, key=key)\\nstate = my_pool.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_pool = hf_roberta.RobertaModel(hf_config, add_pooling_layer=True)\\nhf_pool.load_state_dict(state, strict=True)\\n\\n# my_output = my_pool(input_ids = input_ids, attention_mask=mask, key=key)\\n# hf_output = hf_pool(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\\n\\nmy_output = my_pool(input_embeds = input_embeds, attention_mask=mask, key=key)\\nhf_output = hf_pool(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\\n\\ncheck(my_output[1].array, hf_output[1].detach(), ppp=True)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaModel\n",
    "\n",
    "my_pool = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=True, key=key)\n",
    "state = my_pool.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_pool = hf_roberta.RobertaModel(hf_config, add_pooling_layer=True)\n",
    "hf_pool.load_state_dict(state, strict=True)\n",
    "\n",
    "# my_output = my_pool(input_ids = input_ids, attention_mask=mask, key=key)\n",
    "# hf_output = hf_pool(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "my_output = my_pool(input_embeds = input_embeds, attention_mask=mask, key=key)\n",
    "hf_output = hf_pool(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "check(my_output[1].array, hf_output[1].detach(), ppp=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaLMHead\\n\\nmy_pool = my_roberta.RobertaLMHead.init(Vocab, my_config, key=key)\\nstate = my_pool.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nstate[\"bias\"] = torch.zeros(hf_config.vocab_size)\\n\\nprint(state.keys())\\n\\nhf_pool = hf_roberta.RobertaLMHead(hf_config)\\nhf_pool.load_state_dict(state, strict=True)\\n\\nmy_output = my_pool(features, key=key)\\nhf_output = hf_pool(features_torch)\\n\\ncheck(my_output.array, hf_output.detach(), ppp=True)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaLMHead\n",
    "\n",
    "my_pool = my_roberta.RobertaLMHead.init(Vocab, my_config, key=key)\n",
    "state = my_pool.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "state[\"bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_pool = hf_roberta.RobertaLMHead(hf_config)\n",
    "hf_pool.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_pool(features, key=key)\n",
    "hf_output = hf_pool(features_torch)\n",
    "\n",
    "check(my_output.array, hf_output.detach(), ppp=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaForMaskedLM\\n\\nmy_pool = my_roberta.RobertaForMaskedLM.init(Vocab, my_config, key=key)\\nstate = my_pool.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nstate[\"lm_head.bias\"] = torch.zeros(hf_config.vocab_size)\\n\\nprint(state.keys())\\n\\nhf_pool = hf_roberta.RobertaForMaskedLM(hf_config)\\nhf_pool.load_state_dict(state, strict=True)\\n\\nmy_output_MLM = my_pool(input_ids = input_ids, attention_mask=mask, key=key)\\nhf_output_MLM = hf_pool(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\\n\\n# my_output = my_pool(input_embeds = input_embeds, attention_mask=mask, key=key)\\n# hf_output = hf_pool(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\\n\\ncheck(my_output_MLM[0].array, hf_output_MLM[0].detach(), ppp=True)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaForMaskedLM\n",
    "\n",
    "my_pool = my_roberta.RobertaForMaskedLM.init(Vocab, my_config, key=key)\n",
    "state = my_pool.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "state[\"lm_head.bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_pool = hf_roberta.RobertaForMaskedLM(hf_config)\n",
    "hf_pool.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output_MLM = my_pool(input_ids = input_ids, attention_mask=mask, key=key)\n",
    "hf_output_MLM = hf_pool(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "# my_output = my_pool(input_embeds = input_embeds, attention_mask=mask, key=key)\n",
    "# hf_output = hf_pool(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "check(my_output_MLM[0].array, hf_output_MLM[0].detach(), ppp=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaModel\\n\\nmy_model = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=False, key=key)\\nstate = my_model.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_model = hf_roberta.RobertaModel(hf_config, add_pooling_layer=False)\\nhf_model.load_state_dict(state, strict=True)\\n\\nmy_output = my_model(input_ids = input_ids, attention_mask=mask, key=key)\\nhf_output = hf_model(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\\n\\ncheck(my_output[0].array, hf_output[0].detach(), ppp=True)\\n\\n# Testing RobertaLMHead\\n\\nmy_head = my_roberta.RobertaLMHead.init(Vocab, my_config, key=key)\\nstate = my_head.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nstate[\"bias\"] = torch.zeros(hf_config.vocab_size)\\n\\nprint(state.keys())\\n\\nhf_head = hf_roberta.RobertaLMHead(hf_config)\\nhf_head.load_state_dict(state, strict=True)\\n\\nmy_output = my_head(my_output[0], key=key)\\nhf_output = hf_head(hf_output[0])\\n\\ncheck(my_output.array, hf_output.detach(), ppp=True)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaModel\n",
    "\n",
    "my_model = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=False, key=key)\n",
    "state = my_model.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_model = hf_roberta.RobertaModel(hf_config, add_pooling_layer=False)\n",
    "hf_model.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_model(input_ids = input_ids, attention_mask=mask, key=key)\n",
    "hf_output = hf_model(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "check(my_output[0].array, hf_output[0].detach(), ppp=True)\n",
    "\n",
    "# Testing RobertaLMHead\n",
    "\n",
    "my_head = my_roberta.RobertaLMHead.init(Vocab, my_config, key=key)\n",
    "state = my_head.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "state[\"bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_head = hf_roberta.RobertaLMHead(hf_config)\n",
    "hf_head.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_head(my_output[0], key=key)\n",
    "hf_output = hf_head(hf_output[0])\n",
    "\n",
    "check(my_output.array, hf_output.detach(), ppp=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias'])\n",
      "dict_keys(['encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias'])\n",
      "dict_keys(['dense.weight', 'dense.bias', 'layer_norm.weight', 'layer_norm.bias', 'decoder.weight', 'decoder.bias', 'bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing RobertaForMaskedLM\n",
    "\n",
    "my_mlm = my_roberta.RobertaForMaskedLM.init(Vocab, my_config, key=key)\n",
    "state = my_mlm.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "state[\"lm_head.bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_mlm = hf_roberta.RobertaForMaskedLM(hf_config)\n",
    "hf_mlm.load_state_dict(state, strict=True)\n",
    "\n",
    "# Testing RobertaModel\n",
    "\n",
    "key_rob, key_head = jrandom.split(key, 2)\n",
    "\n",
    "my_model = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=False, key=key_rob)\n",
    "state = my_model.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_model = hf_roberta.RobertaModel(hf_config, add_pooling_layer=False)\n",
    "hf_model.load_state_dict(state, strict=True)\n",
    "\n",
    "# Testing RobertaLMHead\n",
    "\n",
    "my_head = my_roberta.RobertaLMHead.init(Vocab, my_config, key=key_head)\n",
    "state = my_head.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "state[\"bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_head = hf_roberta.RobertaLMHead(hf_config)\n",
    "hf_head.load_state_dict(state, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n",
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\haliax\\core.py:249: UserWarning: Found axis with same name but different size.\n",
      "  warnings.warn(\"Found axis with same name but different size.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking RobertaModel\n",
      "Success!!!\n",
      "Iteration 0, Precision 1:\t1.0\n",
      "Iteration 7, Precision 1e-07:\t0.9986305236816406\n",
      "Iteration 8, Precision 1e-08:\t0.9982020060221354\n",
      "Iteration 14, Precision 1e-14:\t0.9981346130371094\n",
      "Accuracy: 1.0\n",
      "Checking Roberta Model + LM head\n",
      "Success!!!\n",
      "Iteration 0, Precision 1:\t1.0\n",
      "Iteration 6, Precision 1e-06:\t0.9996785785337711\n",
      "Iteration 7, Precision 1e-07:\t0.9964101978265194\n",
      "Iteration 8, Precision 1e-08:\t0.995718628767532\n",
      "Iteration 14, Precision 1e-14:\t0.9956369328496468\n",
      "Accuracy: 1.0\n",
      "Checking MLM\n",
      "Fail :((((\n",
      "Iteration 0, Precision 1:\t1.0\n",
      "Iteration 1, Precision 0.1:\t0.6762516280898737\n",
      "Iteration 2, Precision 0.01:\t0.0813977132137173\n",
      "Iteration 3, Precision 0.001:\t0.00872603715930568\n",
      "Iteration 4, Precision 0.0001:\t0.0014499908298517856\n",
      "Iteration 5, Precision 1e-05:\t0.000723081729334527\n",
      "Iteration 14, Precision 1e-14:\t0.0006434452091415498\n",
      "Accuracy: 0.0014499908298517856\n",
      "Checking my RobertaModel + LM head and MLM\n",
      "Success!!!\n",
      "Iteration 0, Precision 1:\t1.0\n",
      "Iteration 14, Precision 1e-14:\t1.0\n",
      "Accuracy: 1.0\n",
      "Checking hf RobertaModel + LM head and MLM\n",
      "Fail :((((\n",
      "Iteration 0, Precision 1:\t1.0\n",
      "Iteration 1, Precision 0.1:\t0.6762513949505122\n",
      "Iteration 2, Precision 0.01:\t0.0813977132137173\n",
      "Iteration 3, Precision 0.001:\t0.008726678292549488\n",
      "Iteration 4, Precision 0.0001:\t0.0014501462560927087\n",
      "Iteration 5, Precision 1e-05:\t0.0007229263030936039\n"
     ]
    }
   ],
   "source": [
    "k_rob, k_lm = jrandom.split(key, 2)\n",
    "\n",
    "# MLM\n",
    "\n",
    "my_output_mlm = my_mlm(input_ids = input_ids, attention_mask=mask, key=key)\n",
    "hf_output_mlm = hf_mlm(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "# Model + LM\n",
    "\n",
    "my_output_model = my_model(input_ids = input_ids, attention_mask=mask, key=k_rob)\n",
    "my_output = my_head(my_output_model[0], key=k_lm)\n",
    "\n",
    "hf_output_model = hf_model(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "hf_output = hf_head(hf_output_model[0])\n",
    "\n",
    "# # MLM\n",
    "# my_output_mlm = my_mlm(input_embeds = input_embeds, attention_mask=mask, key=key)\n",
    "# hf_output_mlm = hf_mlm(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "# # Model + LM\n",
    "# my_output_model = my_model(input_embeds = input_embeds, attention_mask=mask, key=k_rob)\n",
    "# my_output = my_head(my_output_model[0], key=k_lm)\n",
    "\n",
    "# hf_output_model = hf_model(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\n",
    "# hf_output = hf_head(hf_output_model[0])\n",
    "\n",
    "# Checks\n",
    "\n",
    "print(\"Checking RobertaModel\")\n",
    "check(my_output_model[0].array, hf_output_model[0].detach(), pppp=True)\n",
    "print(\"Checking Roberta Model + LM head\")\n",
    "check(my_output.array, hf_output.detach(), pppp=True)\n",
    "print(\"Checking MLM\")\n",
    "check(my_output_mlm.array, hf_output_mlm[0].detach(), pppp=True)\n",
    "\n",
    "print(\"Checking my RobertaModel + LM head and MLM\")\n",
    "check(my_output.array, my_output_mlm.array, pppp=True)\n",
    "print(\"Checking hf RobertaModel + LM head and MLM\")\n",
    "check(hf_output.detach(), hf_output_mlm[0].detach(), pppp=True)\n",
    "\n",
    "\n",
    "# Notes\n",
    "# embeds works between hf and my, and within model->head and mlm \n",
    "# ids does not work between hf and my for mlm or within hf for model->head and mlm - so hf mlm is doing something weird."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
