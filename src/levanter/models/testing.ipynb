{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-12 14:00:57,074\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import roberta as my_roberta\n",
    "from transformers.models.roberta import modeling_roberta as hf_roberta\n",
    "\n",
    "import torch\n",
    "import haliax as hax\n",
    "import jax.random as jrandom\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from time import time\n",
    "\n",
    "hf_model_str = \"FacebookAI/roberta-base\"\n",
    "\n",
    "hf_config = AutoConfig.from_pretrained(hf_model_str)\n",
    "hf_config.hidden_dropout_prob = 0\n",
    "hf_config.attention_probs_dropout_prob = 0\n",
    "hf_config.pad_token_id = -1\n",
    "my_config = my_roberta.RobertaConfig.from_hf_config(hf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 1723496463\n"
     ]
    }
   ],
   "source": [
    "seed = time()\n",
    "print(f\"seed: {int(seed)}\")\n",
    "key_vars = jrandom.PRNGKey(int(seed))\n",
    "\n",
    "EmbedAtt = my_config.EmbedAtt\n",
    "Embed = my_config.Embed\n",
    "Mlp = my_config.Mlp\n",
    "Pos = my_config.Pos\n",
    "KeyPos = my_config.KeyPos\n",
    "Heads = my_config.Heads\n",
    "\n",
    "Batch = hax.Axis(\"batch\", 2)\n",
    "Vocab = hax.Axis(\"vocab\", my_config.vocab_size)\n",
    "\n",
    "keys = jrandom.split(key_vars, 6)\n",
    "\n",
    "input_ids = hax.random.randint(keys[0], (Batch, Pos), minval = 3, maxval = my_config.vocab_size)\n",
    "input_ids_torch = torch.from_numpy(np.array(input_ids.array))\n",
    "input_embeds = hax.random.normal(keys[1], (Batch, Pos, Embed))\n",
    "input_embeds_torch = torch.from_numpy(np.array(input_embeds.array))\n",
    "\n",
    "# mask = hax.random.randint(keys[2], (Batch, Pos), minval = 0, maxval = 2)\n",
    "mask = hax.ones((Batch, Pos))\n",
    "mask_torch = torch.from_numpy(np.array(mask.array))\n",
    "mask_torch_materialized = torch.ones((2, hf_config.num_attention_heads, hf_config.max_position_embeddings, hf_config.max_position_embeddings))\n",
    "\n",
    "features = input_embeds[{\"position\": 0}]\n",
    "features_torch = torch.from_numpy(np.array(features.array))\n",
    "\n",
    "x_embed_att = input_embeds.rename({\"embed\": \"embed_att\"})\n",
    "x_embed_att_torch = torch.from_numpy(np.array(x_embed_att.array))\n",
    "x_mlp = hax.random.normal(keys[5], (Batch, Pos, Mlp))\n",
    "x_mlp_torch = torch.from_numpy(np.array(x_mlp.array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Random Mask causes RobertaModel to have different output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(my_output, hf_output, p=False, pp=False, ppp=False, pppp=True, precision=1e-4):\n",
    "    \n",
    "    assert (np.array(my_output.shape) == np.array(hf_output.shape)).all()\n",
    "    # print(my_output.shape)\n",
    "    # print(hf_output.shape)\n",
    "\n",
    "    acc = np.isclose(hf_output, np.array(my_output), rtol=precision, atol=precision).mean()\n",
    "\n",
    "    stats = (torch.tensor(np.array(my_output)).abs().mean(), torch.tensor(np.array(hf_output)).abs().mean())\n",
    "    \n",
    "    if p:   \n",
    "        acc = np.isclose(hf_output, np.array(my_output), rtol=precision, atol=precision).mean()\n",
    "        print(f\"Accuracy: {acc}\")\n",
    "        print(f\"Jax:\\n{torch.tensor(np.array(my_output))}\\nTorch:\\n{hf_output}\")\n",
    "\n",
    "    if pp:\n",
    "        diff = torch.tensor(np.array(my_output)) - hf_output\n",
    "        print(f\"Mean: {diff.abs().mean()}\")\n",
    "        print(f\"Stdev: {diff.std()}\")\n",
    "        print(f\"Difference:\\n{diff}\")\n",
    "\n",
    "    if ppp:\n",
    "        acc_prev = None\n",
    "        for i in range(15):\n",
    "            prec = 10 ** (-1*i)\n",
    "            acc = np.isclose(hf_output, np.array(my_output), rtol=precision, atol=prec).mean()\n",
    "            if acc_prev is None:\n",
    "                print(f\"Iteration {i}, Precision {prec}:\\t{acc}\")\n",
    "            else:\n",
    "                if np.abs(acc - acc_prev) > 1e-4:\n",
    "                    print(f\"Iteration {i}, Precision {prec}:\\t{acc}\")\n",
    "            acc_prev = acc\n",
    "        print(f\"Iteration {i}, Precision {prec}:\\t{acc}\")\n",
    "    \n",
    "    return acc, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing RobertaSelfOutput\n",
    "\n",
    "def test_RobertaSelfOutput(key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaSelfOutput.init(my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaSelfOutput(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "    my_output = my_func(x_embed_att, input_embeds, key=k_2)\n",
    "    hf_output = hf_func(x_embed_att_torch, input_embeds_torch)\n",
    "\n",
    "    return check(my_output.array, hf_output.detach())\n",
    "\n",
    "# Testing RobertaSelfAttention\n",
    "\n",
    "def test_RobertaSelfAttention(key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "\n",
    "    my_func = my_roberta.RobertaSelfAttention.init(my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func  = hf_roberta.RobertaSelfAttention(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "    my_output = my_func(input_embeds, mask, key=k_2)\n",
    "    hf_output = hf_func(input_embeds_torch, mask_torch_materialized)\n",
    "\n",
    "    return check(my_output.array, hf_output[0].detach())\n",
    "\n",
    "# Testing RobertaAttention\n",
    "\n",
    "def test_RobertaAttention(key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaAttention.init(my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaAttention(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "    my_output = my_func(hidden_states=input_embeds, attention_mask=mask, key=k_2)\n",
    "    hf_output = hf_func(hidden_states=input_embeds_torch, attention_mask=mask_torch_materialized)\n",
    "\n",
    "    return check(my_output.array, hf_output[0].detach())\n",
    "\n",
    "# Testing RobertaIntermediate\n",
    "\n",
    "def test_RobertaIntermediate(key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaIntermediate.init(my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaIntermediate(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "    my_output = my_func(input_embeds, key=k_2)\n",
    "    hf_output = hf_func(input_embeds_torch)\n",
    "\n",
    "    return check(my_output.array, hf_output.detach())\n",
    "\n",
    "# Testing RobertaOutput\n",
    "\n",
    "def test_RobertaOutput(key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "\n",
    "    my_func = my_roberta.RobertaOutput.init(my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaOutput(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "    my_output = my_func(x_mlp, input_embeds, key=k_2)\n",
    "    hf_output = hf_func(x_mlp_torch, input_embeds_torch)\n",
    "\n",
    "    return check(my_output.array, hf_output.detach())\n",
    "\n",
    "# Testing RobertaLayer\n",
    "\n",
    "def test_RobertaLayer(key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaLayer.init(my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaLayer(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "    my_output = my_func(hidden_states=input_embeds, attention_mask=mask, key=k_2)\n",
    "    hf_output = hf_func(hidden_states=input_embeds_torch, attention_mask=mask_torch_materialized)\n",
    "\n",
    "    return check(my_output.array, hf_output[0].detach())\n",
    "\n",
    "# Testing RobertaEncoder\n",
    "\n",
    "def test_RobertaEncoder(key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaEncoder.init(my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaEncoder(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "    my_output = my_func(hidden_states=input_embeds, attention_mask=mask, key=k_2)\n",
    "    hf_output = hf_func(hidden_states=input_embeds_torch, attention_mask=mask_torch_materialized)\n",
    "\n",
    "    return check(my_output.array, hf_output[0].detach())\n",
    "\n",
    "# Testing RobertaEmbedding\n",
    "\n",
    "def test_RobertaEmbedding(key, ids = True):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaEmbedding.init(Vocab, my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaEmbeddings(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "    if ids:\n",
    "        my_output = my_func.embed(input_ids=input_ids, key=k_2)\n",
    "        hf_output = hf_func(input_ids=input_ids_torch)\n",
    "    else:        \n",
    "        my_output = my_func.embed(input_embeds=input_embeds, key=k_2)\n",
    "        hf_output = hf_func(inputs_embeds=input_embeds_torch)\n",
    "\n",
    "    return check(my_output.array, hf_output.detach())\n",
    "\n",
    "# Testing RobertaPooler\n",
    "\n",
    "def test_RobertaPooler(key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaPooler.init(my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaPooler(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "    my_output = my_func(input_embeds, key=k_2)\n",
    "    hf_output = hf_func(input_embeds_torch)\n",
    "\n",
    "    return check(my_output.array, hf_output.detach())\n",
    "\n",
    "# Testing RobertaModel\n",
    "\n",
    "def test_RobertaModel(key, ids = True, pool = True):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=pool, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaModel(hf_config, add_pooling_layer=pool)\n",
    "    hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "    if ids:\n",
    "        my_output = my_func(input_ids = input_ids, attention_mask=mask, key=k_2)\n",
    "        hf_output = hf_func(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "    else:\n",
    "        my_output = my_func(input_embeds = input_embeds, attention_mask=mask, key=k_2)\n",
    "        hf_output = hf_func(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "    if pool:\n",
    "        return check(my_output[1].array, hf_output[1].detach())\n",
    "    else:\n",
    "        return check(my_output[0].array, hf_output[0].detach())\n",
    "\n",
    "# Testing RobertaLMHead\n",
    "\n",
    "def test_RobertaLMHead(key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaLMHead.init(Vocab, my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    state[\"bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaLMHead(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "    my_output = my_func(features, key=k_2)\n",
    "    hf_output = hf_func(features_torch)\n",
    "\n",
    "    return check(my_output.array, hf_output.detach())\n",
    "\n",
    "# Testing RobertaForMaskedLM\n",
    "\n",
    "def test_RobertaForMaskedLM(key, ids = True):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_pool = my_roberta.RobertaForMaskedLM.init(Vocab, my_config, key=k_1)\n",
    "    state = my_pool.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    state[\"lm_head.bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_pool = hf_roberta.RobertaForMaskedLM(hf_config)\n",
    "    hf_pool.load_state_dict(state, strict=True)\n",
    "\n",
    "    if ids:\n",
    "        my_output = my_pool(input_ids = input_ids, attention_mask=mask, key=k_2)\n",
    "        hf_output = hf_pool(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "    else:\n",
    "        my_output = my_pool(input_embeds = input_embeds, attention_mask=mask, key=k_2)\n",
    "        hf_output = hf_pool(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "    return check(my_output.array, hf_output[0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_func(input):\n",
    "    acc, stats = input\n",
    "    if acc < 1:\n",
    "        return str(acc) + \"\\t<---- here\"\n",
    "    else:\n",
    "        return str(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = time() + 20\n",
    "# print(f\"seed: {int(seed)}\")\n",
    "# key_vars = jrandom.PRNGKey(int(seed))\n",
    "# keys = jrandom.split(key_vars, 15)\n",
    "\n",
    "# print(f\"test_RobertaSelfOutput: {out_func(test_RobertaSelfOutput(keys[0]))}\")\n",
    "# print(f\"test_RobertaSelfAttention: {out_func(test_RobertaSelfAttention(keys[1]))}\")\n",
    "# print(f\"test_RobertaAttention: {out_func(test_RobertaAttention(keys[2]))}\")\n",
    "# print(f\"test_RobertaIntermediate: {out_func(test_RobertaIntermediate(keys[3]))}\")\n",
    "# print(f\"test_RobertaOutput: {out_func(test_RobertaOutput(keys[4]))}\")\n",
    "# print(f\"test_RobertaEmbedding(ids = True): {out_func(test_RobertaEmbedding(keys[7], ids = True))}\")\n",
    "# print(f\"test_RobertaEmbedding(ids = False): {out_func(test_RobertaEmbedding(keys[8], ids = False))}\")\n",
    "# print(f\"test_RobertaModel(ids = True, pool = True): {out_func(test_RobertaModel(keys[9], ids = True, pool = True))}\")\n",
    "# print(f\"test_RobertaModel(ids = False, pool = False): {out_func(test_RobertaModel(keys[10], ids = False, pool = False))}\")\n",
    "# print(f\"test_RobertaModel(ids = True, pool = True): {out_func(test_RobertaModel(keys[9], ids = True, pool = True))}\")\n",
    "# print(f\"test_RobertaModel(ids = False, pool = False): {out_func(test_RobertaModel(keys[10], ids = False, pool = False))}\")\n",
    "# print(f\"test_RobertaPooler: {out_func(test_RobertaPooler(keys[11]))}\")\n",
    "# print(f\"test_RobertaLMHead: {out_func(test_RobertaLMHead(keys[12]))}\")\n",
    "# print(f\"test_RobertaForMaskedLM(ids = True): {out_func(test_RobertaForMaskedLM(keys[13], ids = True))}\")\n",
    "# print(f\"test_RobertaForMaskedLM(ids = False): {out_func(test_RobertaForMaskedLM(keys[14], ids = False))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_RobertaEmbedding(input, key, ids = True):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaEmbedding.init(Vocab, my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaEmbeddings(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "    input_torch = torch.from_numpy(np.array(input.array))\n",
    "\n",
    "    if ids:\n",
    "        my_output = my_func.embed(input_ids=input, key=k_2)\n",
    "        hf_output = hf_func(input_ids=input_torch)\n",
    "    else:        \n",
    "        my_output = my_func.embed(input_embeds=input, key=k_2)\n",
    "        hf_output = hf_func(inputs_embeds=input_torch)\n",
    "\n",
    "    return check(my_output.array, hf_output.detach()), (my_output, hf_output)\n",
    "\n",
    "def get_output_RobertaEncoder(input, key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaEncoder.init(my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaEncoder(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "    input_torch = torch.from_numpy(np.array(input.array))\n",
    "\n",
    "    attention_mask = hax.ones((Batch, Heads, Pos, KeyPos)) * -jnp.inf\n",
    "    attention_mask_torch = torch.from_numpy(np.array(attention_mask.array))\n",
    "\n",
    "    my_output = my_func(hidden_states=input, attention_mask=attention_mask, key=k_2)\n",
    "    hf_output = hf_func(hidden_states=input_torch, attention_mask=attention_mask_torch)\n",
    "\n",
    "    return check(my_output.array, hf_output[0].detach()), (my_output, hf_output)\n",
    "\n",
    "def get_output_RobertaPooler(input, key):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaPooler.init(my_config, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaPooler(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "    input_torch = torch.from_numpy(np.array(input.array))\n",
    "\n",
    "    my_output = my_func(input, key=k_2)\n",
    "    hf_output = hf_func(input_torch)\n",
    "\n",
    "    return check(my_output.array, hf_output.detach()), (my_output, hf_output)\n",
    "\n",
    "# Testing RobertaModel\n",
    "\n",
    "def get_output_RobertaModel(input, key, ids = True, pool = True):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=pool, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaModel(hf_config, add_pooling_layer=pool)\n",
    "    hf_func.load_state_dict(state, strict=True)\n",
    "    \n",
    "    input_torch = torch.from_numpy(np.array(input.array))\n",
    "    \n",
    "    # attention_mask = hax.ones((Batch, Heads, Pos, KeyPos)) * -jnp.inf\n",
    "    # attention_mask_torch = torch.from_numpy(np.array(attention_mask.array))\n",
    "\n",
    "    if ids:\n",
    "        my_output = my_func(input_ids = input, attention_mask=mask, key=k_2)\n",
    "        hf_output = hf_func(input_ids = input_torch, attention_mask=mask_torch, return_dict=False)\n",
    "    else:\n",
    "        my_output = my_func(input_embeds = input, attention_mask=mask, key=k_2)\n",
    "        hf_output = hf_func(inputs_embeds = input_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "    if pool:\n",
    "        return check(my_output[1].array, hf_output[1].detach()), (my_output, hf_output)\n",
    "    else:\n",
    "        return check(my_output[0].array, hf_output[0].detach()), (my_output, hf_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 1723505034\n",
      "{'batch': 2, 'position': 514, 'embed': 768}\n",
      "(tensor(0.7984), tensor(0.7984))\n",
      "(tensor(nan), tensor(nan))\n",
      "(tensor(nan), tensor(nan))\n",
      "(tensor(0.5408), tensor(0.5408))\n",
      "acc_embeds: 1.0\n",
      "acc_enc: 0.0\n",
      "acc_pool: 0.0\n",
      "acc_model: 1.0\n",
      "my comparison pool: (0.0, (tensor(nan), tensor(0.5408)))\n",
      "my comparison no pool: (0.0, (tensor(nan), tensor(0.7873)))\n",
      "hf comparison: (0.0, (tensor(nan), tensor(0.5408)))\n"
     ]
    }
   ],
   "source": [
    "seed = time() + 30\n",
    "print(f\"seed: {int(seed)}\")\n",
    "key = jrandom.PRNGKey(int(seed))\n",
    "\n",
    "k_t, k_emb, k_p = jrandom.split(key, 3)\n",
    "\n",
    "input = input_embeds\n",
    "\n",
    "(acc_embeds, stats_embed), (my_out_embeds, hf_out_embeds) = get_output_RobertaEmbedding(input, k_t, ids = False)\n",
    "print(stats_embed)\n",
    "(acc_enc, stats_enc), (my_out_enc, hf_out_enc) = get_output_RobertaEncoder(my_out_embeds, k_emb)\n",
    "print(stats_enc)\n",
    "(acc_pool, stats_pool), (my_out_pool, hf_out_pool) = get_output_RobertaPooler(my_out_enc, k_p)\n",
    "print(stats_pool)\n",
    "\n",
    "(acc_model, stats_model), (my_out_model, hf_out_model) = get_output_RobertaModel(input, key, ids = False, pool = True)\n",
    "print(stats_model)\n",
    "\n",
    "print(f\"acc_embeds: {acc_embeds}\")\n",
    "print(f\"acc_enc: {acc_enc}\")\n",
    "print(f\"acc_pool: {acc_pool}\")\n",
    "print(f\"acc_model: {acc_model}\")\n",
    "print(f\"my comparison pool: {check(my_out_pool.array, my_out_model[1].array)}\")\n",
    "print(f\"my comparison no pool: {check(my_out_enc.array, my_out_model[0].array)}\")\n",
    "print(f\"hf comparison: {check(hf_out_pool.detach(), hf_out_model[1].detach())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NamedArray(float32{'batch': 2, 'embed': 768},\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]])\n",
      "NamedArray(float32{'batch': 2, 'position': 514, 'embed': 768},\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]])\n",
      "NamedArray(float32{'batch': 2, 'position': 514, 'embed': 768},\n",
      "[[[ 0.47864354  1.2938721   1.0534003  ... -1.4044254   0.829634\n",
      "    0.00428176]\n",
      "  [-0.98862374 -0.943986   -1.0448135  ... -0.64666593  0.12967904\n",
      "   -1.0975188 ]\n",
      "  [ 0.43071395 -0.60738516 -1.7641208  ... -1.1334671   0.9041689\n",
      "    0.9875958 ]\n",
      "  ...\n",
      "  [ 1.287216    0.507795    0.23451686 ... -0.9582702  -0.3576718\n",
      "    0.6565546 ]\n",
      "  [ 0.33264828 -0.68922603  0.41440547 ...  0.4528543  -0.6819962\n",
      "    0.4289952 ]\n",
      "  [ 0.725326    1.9756228   1.1881577  ...  0.5643402   0.5135605\n",
      "    0.92514485]]\n",
      "\n",
      " [[ 1.0026733   1.1235753  -1.017235   ... -1.8810284  -0.29097554\n",
      "   -0.63098675]\n",
      "  [-0.47498116  1.9341669   0.23969549 ... -0.45160082 -0.955768\n",
      "   -1.4716814 ]\n",
      "  [-0.2948639   0.25138515 -1.3983693  ... -0.96624637  0.44848248\n",
      "   -0.71705264]\n",
      "  ...\n",
      "  [-0.4495727   0.07491604  0.919175   ...  0.565745   -0.34500855\n",
      "   -1.9166113 ]\n",
      "  [-0.3112308  -0.21019831  0.2379393  ...  1.3521733   0.1243041\n",
      "   -1.3730545 ]\n",
      "  [-0.9592797  -1.1558015  -1.3304269  ...  1.4129258   0.69931823\n",
      "    0.24171986]]])\n",
      "(NamedArray(array=Array([[[-0.18572666, -0.80562836, -0.98205453, ...,  0.9597818 ,\n",
      "          1.5924176 ,  0.3848163 ],\n",
      "        [-0.19698945, -0.7534842 , -0.81553775, ...,  0.8907304 ,\n",
      "          1.6596173 ,  0.37368602],\n",
      "        [-0.14524579, -0.6914802 , -0.91773754, ...,  1.024012  ,\n",
      "          1.6283392 ,  0.34325117],\n",
      "        ...,\n",
      "        [-0.12597106, -0.76580876, -0.8392121 , ...,  0.9352241 ,\n",
      "          1.5550641 ,  0.46660298],\n",
      "        [-0.17409518, -0.749031  , -1.056306  , ...,  0.9757236 ,\n",
      "          1.633118  ,  0.5897971 ],\n",
      "        [-0.30729672, -0.69016093, -0.87607175, ...,  0.874229  ,\n",
      "          1.6674999 ,  0.38814685]],\n",
      "\n",
      "       [[-0.2995884 , -0.92027843, -0.78937566, ...,  0.43273145,\n",
      "          1.0177305 ,  0.4611196 ],\n",
      "        [-0.3179962 , -0.8442052 , -0.75374943, ...,  0.7022148 ,\n",
      "          1.0696493 ,  0.3404984 ],\n",
      "        [-0.33889046, -0.90215874, -0.65796405, ...,  0.5069377 ,\n",
      "          1.0210142 ,  0.30466717],\n",
      "        ...,\n",
      "        [-0.47294238, -0.8416684 , -0.7532904 , ...,  0.46125498,\n",
      "          1.1491499 ,  0.41495347],\n",
      "        [-0.28819865, -0.8842407 , -0.69517034, ...,  0.49842533,\n",
      "          1.0367949 ,  0.58008623],\n",
      "        [-0.31019112, -0.90532184, -0.7528029 , ...,  0.5191115 ,\n",
      "          1.299418  ,  0.43962964]]], dtype=float32), axes=(Axis(name='batch', size=2), Axis(name='position', size=514), Axis(name='embed', size=768))), NamedArray(array=Array([[ 0.60376894,  0.34222102, -0.01021165, ..., -0.90009135,\n",
      "        -0.6602305 ,  0.14456   ],\n",
      "       [ 0.8103463 ,  0.4916969 , -0.01769697, ..., -0.94779646,\n",
      "        -0.8301279 ,  0.29279563]], dtype=float32), axes=(Axis(name='batch', size=2), Axis(name='embed', size=768))))\n"
     ]
    }
   ],
   "source": [
    "print(my_out_pool)\n",
    "print(my_out_enc)\n",
    "print(my_out_embeds)\n",
    "print(my_out_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaModel\\n\\nmy_model = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=False, key=key)\\nstate = my_model.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\n# print(state.keys())\\n\\nhf_model = hf_roberta.RobertaModel(hf_config, add_pooling_layer=False)\\nhf_model.load_state_dict(state, strict=True)\\n\\nmy_output = my_model(input_ids = input_ids, attention_mask=mask, key=key)\\nhf_output = hf_model(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\\n\\ncheck(my_output[0].array, hf_output[0].detach(), ppp=True)\\n\\n# Testing RobertaLMHead\\n\\nmy_head = my_roberta.RobertaLMHead.init(Vocab, my_config, key=key)\\nstate = my_head.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nstate[\"bias\"] = torch.zeros(hf_config.vocab_size)\\n\\n# print(state.keys())\\n\\nhf_head = hf_roberta.RobertaLMHead(hf_config)\\nhf_head.load_state_dict(state, strict=True)\\n\\nmy_output = my_head(my_output[0], key=key)\\nhf_output = hf_head(hf_output[0])\\n\\ncheck(my_output.array, hf_output.detach(), ppp=True)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaModel\n",
    "\n",
    "my_model = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=False, key=key)\n",
    "state = my_model.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "# print(state.keys())\n",
    "\n",
    "hf_model = hf_roberta.RobertaModel(hf_config, add_pooling_layer=False)\n",
    "hf_model.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_model(input_ids = input_ids, attention_mask=mask, key=key)\n",
    "hf_output = hf_model(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "check(my_output[0].array, hf_output[0].detach(), ppp=True)\n",
    "\n",
    "# Testing RobertaLMHead\n",
    "\n",
    "my_head = my_roberta.RobertaLMHead.init(Vocab, my_config, key=key)\n",
    "state = my_head.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "state[\"bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "\n",
    "# print(state.keys())\n",
    "\n",
    "hf_head = hf_roberta.RobertaLMHead(hf_config)\n",
    "hf_head.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_head(my_output[0], key=key)\n",
    "hf_output = hf_head(hf_output[0])\n",
    "\n",
    "check(my_output.array, hf_output.detach(), ppp=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaForMaskedLM\\n\\nmy_mlm = my_roberta.RobertaForMaskedLM.init(Vocab, my_config, key=key)\\nstate_mlm = my_mlm.to_state_dict()\\n\\nstate_mlm = {k: torch.from_numpy(np.array(v)) for k, v in state_mlm.items()}\\n\\n# if \"lm_head.decoder.bias\" in state:\\n#     print(state[\"lm_head.decoder.bias\"])\\n# else:\\n#     print(f\"RobertaForMaskedLM, {state.keys()}\")\\n\\nstate_mlm[\"lm_head.bias\"] = torch.zeros(hf_config.vocab_size)\\n\\nprint(state_mlm.keys())\\n\\nhf_mlm = hf_roberta.RobertaForMaskedLM(hf_config)\\nhf_mlm.load_state_dict(state_mlm, strict=True)\\n\\n# Testing RobertaModel\\n\\nkey_rob, key_head = jrandom.split(key, 2)\\n\\nmy_model = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=False, key=key_rob)\\nstate_model = my_model.to_state_dict()\\n\\nstate_model = {k: torch.from_numpy(np.array(v)) for k, v in state_model.items()}\\n\\nprint(state_model.keys())\\n\\nhf_model = hf_roberta.RobertaModel(hf_config, add_pooling_layer=False)\\nhf_model.load_state_dict(state_model, strict=True)\\n\\n# Testing RobertaLMHead\\n\\nmy_head = my_roberta.RobertaLMHead.init(Vocab, my_config, key=key_head)\\nstate_head = my_head.to_state_dict()\\n\\nstate_head = {k: torch.from_numpy(np.array(v)) for k, v in state_head.items()}\\n\\nstate_head[\"bias\"] = torch.zeros(hf_config.vocab_size)\\n\\nprint(state_head.keys())\\n\\nhf_head = hf_roberta.RobertaLMHead(hf_config)\\nhf_head.load_state_dict(state_head, strict=True)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaForMaskedLM\n",
    "\n",
    "my_mlm = my_roberta.RobertaForMaskedLM.init(Vocab, my_config, key=key)\n",
    "state_mlm = my_mlm.to_state_dict()\n",
    "\n",
    "state_mlm = {k: torch.from_numpy(np.array(v)) for k, v in state_mlm.items()}\n",
    "\n",
    "# if \"lm_head.decoder.bias\" in state:\n",
    "#     print(state[\"lm_head.decoder.bias\"])\n",
    "# else:\n",
    "#     print(f\"RobertaForMaskedLM, {state.keys()}\")\n",
    "\n",
    "state_mlm[\"lm_head.bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "\n",
    "print(state_mlm.keys())\n",
    "\n",
    "hf_mlm = hf_roberta.RobertaForMaskedLM(hf_config)\n",
    "hf_mlm.load_state_dict(state_mlm, strict=True)\n",
    "\n",
    "# Testing RobertaModel\n",
    "\n",
    "key_rob, key_head = jrandom.split(key, 2)\n",
    "\n",
    "my_model = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=False, key=key_rob)\n",
    "state_model = my_model.to_state_dict()\n",
    "\n",
    "state_model = {k: torch.from_numpy(np.array(v)) for k, v in state_model.items()}\n",
    "\n",
    "print(state_model.keys())\n",
    "\n",
    "hf_model = hf_roberta.RobertaModel(hf_config, add_pooling_layer=False)\n",
    "hf_model.load_state_dict(state_model, strict=True)\n",
    "\n",
    "# Testing RobertaLMHead\n",
    "\n",
    "my_head = my_roberta.RobertaLMHead.init(Vocab, my_config, key=key_head)\n",
    "state_head = my_head.to_state_dict()\n",
    "\n",
    "state_head = {k: torch.from_numpy(np.array(v)) for k, v in state_head.items()}\n",
    "\n",
    "state_head[\"bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "\n",
    "print(state_head.keys())\n",
    "\n",
    "hf_head = hf_roberta.RobertaLMHead(hf_config)\n",
    "hf_head.load_state_dict(state_head, strict=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'k_rob, k_lm = jrandom.split(key, 2)\\n\\n# MLM\\n\\nmy_output_mlm = my_mlm(input_ids = input_ids, attention_mask=mask, key=key)\\nhf_output_mlm = hf_mlm(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\\n\\n# Model + LM\\n\\nmy_output_model = my_model(input_ids = input_ids, attention_mask=mask, key=k_rob)\\nmy_output = my_head(my_output_model[0], key=k_lm)\\n\\nhf_output_model = hf_model(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\\nhf_output = hf_head(hf_output_model[0])\\n\\n# # MLM\\n# my_output_mlm = my_mlm(input_embeds = input_embeds, attention_mask=mask, key=key)\\n# hf_output_mlm = hf_mlm(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\\n\\n# # Model + LM\\n# my_output_model = my_model(input_embeds = input_embeds, attention_mask=mask, key=k_rob)\\n# my_output = my_head(my_output_model[0], key=k_lm)\\n\\n# hf_output_model = hf_model(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\\n# hf_output = hf_head(hf_output_model[0])\\n\\n# Checks\\n\\nprint(\"\\nChecking RobertaModel\")\\ncheck(my_output_model[0].array, hf_output_model[0].detach(), pppp=True)\\nprint(\"\\nChecking Roberta Model + LM head\")\\ncheck(my_output.array, hf_output.detach(), pppp=True)\\nprint(\"\\nChecking MLM\")\\ncheck(my_output_mlm.array, hf_output_mlm[0].detach(), pppp=True)\\n\\nprint(\"\\nChecking my RobertaModel + LM head and MLM\")\\ncheck(my_output.array, my_output_mlm.array, pppp=True)\\nprint(\"\\nChecking hf RobertaModel + LM head and MLM\")\\ncheck(hf_output.detach(), hf_output_mlm[0].detach(), pppp=True)\\n\\n\\n# Notes\\n# embeds works between hf and my, and within model->head and mlm \\n# ids does not work between hf and my for mlm or within hf for model->head and mlm - so hf mlm is doing something weird.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''k_rob, k_lm = jrandom.split(key, 2)\n",
    "\n",
    "# MLM\n",
    "\n",
    "my_output_mlm = my_mlm(input_ids = input_ids, attention_mask=mask, key=key)\n",
    "hf_output_mlm = hf_mlm(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "# Model + LM\n",
    "\n",
    "my_output_model = my_model(input_ids = input_ids, attention_mask=mask, key=k_rob)\n",
    "my_output = my_head(my_output_model[0], key=k_lm)\n",
    "\n",
    "hf_output_model = hf_model(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "hf_output = hf_head(hf_output_model[0])\n",
    "\n",
    "# # MLM\n",
    "# my_output_mlm = my_mlm(input_embeds = input_embeds, attention_mask=mask, key=key)\n",
    "# hf_output_mlm = hf_mlm(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "# # Model + LM\n",
    "# my_output_model = my_model(input_embeds = input_embeds, attention_mask=mask, key=k_rob)\n",
    "# my_output = my_head(my_output_model[0], key=k_lm)\n",
    "\n",
    "# hf_output_model = hf_model(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\n",
    "# hf_output = hf_head(hf_output_model[0])\n",
    "\n",
    "# Checks\n",
    "\n",
    "print(\"\\nChecking RobertaModel\")\n",
    "check(my_output_model[0].array, hf_output_model[0].detach(), pppp=True)\n",
    "print(\"\\nChecking Roberta Model + LM head\")\n",
    "check(my_output.array, hf_output.detach(), pppp=True)\n",
    "print(\"\\nChecking MLM\")\n",
    "check(my_output_mlm.array, hf_output_mlm[0].detach(), pppp=True)\n",
    "\n",
    "print(\"\\nChecking my RobertaModel + LM head and MLM\")\n",
    "check(my_output.array, my_output_mlm.array, pppp=True)\n",
    "print(\"\\nChecking hf RobertaModel + LM head and MLM\")\n",
    "check(hf_output.detach(), hf_output_mlm[0].detach(), pppp=True)\n",
    "\n",
    "\n",
    "# Notes\n",
    "# embeds works between hf and my, and within model->head and mlm \n",
    "# ids does not work between hf and my for mlm or within hf for model->head and mlm - so hf mlm is doing something weird.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
