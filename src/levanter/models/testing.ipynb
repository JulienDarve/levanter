{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-30 10:42:02,733\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import roberta as my_roberta\n",
    "from transformers.models.roberta import modeling_roberta as hf_roberta\n",
    "\n",
    "import torch\n",
    "import haliax as hax\n",
    "import jax.random as jrandom\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "hf_model_str = \"FacebookAI/roberta-base\"\n",
    "\n",
    "hf_config = AutoConfig.from_pretrained(hf_model_str)\n",
    "hf_config.hidden_dropout_prob = 0\n",
    "hf_config.attention_probs_dropout_prob = 0\n",
    "my_config = my_roberta.RobertaConfig.from_hf_config(hf_config)\n",
    "\n",
    "key = jrandom.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmbedAtt = my_config.EmbedAtt\n",
    "Embed = my_config.Embed\n",
    "Mlp = my_config.Mlp\n",
    "Pos = my_config.Pos\n",
    "KeyPos = my_config.KeyPos\n",
    "Batch = hax.Axis(\"batch\", 2)\n",
    "\n",
    "x_embed = hax.random.normal(key, (Batch, Pos, Embed))\n",
    "x_embed_att = hax.random.normal(key, (Batch, Pos, EmbedAtt))\n",
    "x_mlp = hax.random.normal(key, (Batch, Pos, Mlp))\n",
    "# x = x[{\"position\": slice(Pos.size-2)}]\n",
    "\n",
    "x_embed_torch = torch.from_numpy(np.array(x_embed.array))\n",
    "x_embed_att_torch = torch.from_numpy(np.array(x_embed_att.array))\n",
    "x_mlp_torch = torch.from_numpy(np.array(x_mlp.array))\n",
    "\n",
    "mask = hax.ones((Batch, Pos))[{\"position\": slice(0,-2)}]\n",
    "mask_torch = torch.from_numpy(np.array(mask.array))\n",
    "# mask_torch = torch.ones((2, hf_config.num_attention_heads, hf_config.max_position_embeddings -2, hf_config.max_position_embeddings -2))\n",
    "\n",
    "Vocab = hax.Axis(\"vocab\", my_config.vocab_size)\n",
    "\n",
    "input_embeds = hax.random.uniform(key, (Batch, Pos, Embed))\n",
    "input_embeds = input_embeds[{\"position\": slice(0,-2)}]\n",
    "\n",
    "input_ids = hax.random.randint(key, (Batch, Pos), minval = 0, maxval = my_config.vocab_size)\n",
    "input_ids = input_ids[{\"position\": slice(0,-2)}]\n",
    "\n",
    "input_embeds_torch = torch.from_numpy(np.array(input_embeds.array))\n",
    "input_ids_torch = torch.from_numpy(np.array(input_ids.array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(hf_model_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(my_output, hf_output, p=False, pp=False, ppp=True, precision=1e-4):\n",
    "    \n",
    "    print(my_output.shape)\n",
    "    print(hf_output.shape)\n",
    "\n",
    "    success = np.isclose(hf_output, np.array(my_output), rtol=precision, atol=precision).all()\n",
    "\n",
    "    if success:\n",
    "        print(\"Success!!!\")\n",
    "    else:\n",
    "        print(\"Fail :((((\")\n",
    "    \n",
    "    if ppp:\n",
    "        acc_prev = None\n",
    "        for i in range(15):\n",
    "            prec = 10 ** (-1*i)\n",
    "            acc = np.isclose(hf_output, np.array(my_output), rtol=precision, atol=prec).mean()\n",
    "            if acc != acc_prev:\n",
    "                print(f\"Iteration {i}, Precision {prec}:\\t{acc}\")\n",
    "            acc_prev = acc\n",
    "\n",
    "    if p:   \n",
    "        acc = np.isclose(hf_output, np.array(my_output), rtol=precision, atol=precision).mean()\n",
    "        print(f\"Accuracy: {acc}\")\n",
    "        print(f\"Jax:\\n{torch.tensor(np.array(my_output))}\\nTorch:\\n{hf_output}\")\n",
    "\n",
    "    if pp:\n",
    "        diff = torch.tensor(np.array(my_output)) - hf_output\n",
    "        print(f\"Mean: {diff.abs().mean()}\")\n",
    "        print(f\"Stdev: {diff.std()}\")\n",
    "        print(f\"Difference:\\n{diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaSelfOutput\\n\\nmy_self_output = my_roberta.RobertaSelfOutput.init(my_config, key=key)\\nstate = my_self_output.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_self_output = hf_roberta.RobertaSelfOutput(hf_config)\\nhf_self_output.load_state_dict(state, strict=True)\\n\\nmy_output = my_self_output(x_embed_att, x_embed, key=key)\\nhf_output = hf_self_output(x_embed_att_torch, x_embed_torch)\\n\\ncheck(my_output.array, hf_output.detach(), ppp=True)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaSelfOutput\n",
    "\n",
    "my_self_output = my_roberta.RobertaSelfOutput.init(my_config, key=key)\n",
    "state = my_self_output.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_self_output = hf_roberta.RobertaSelfOutput(hf_config)\n",
    "hf_self_output.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_self_output(x_embed_att, x_embed, key=key)\n",
    "hf_output = hf_self_output(x_embed_att_torch, x_embed_torch)\n",
    "\n",
    "check(my_output.array, hf_output.detach(), ppp=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaSelfAttention\\n\\nmy_attn_output = my_roberta.RobertaSelfAttention.init(my_config, key=key)\\nstate = my_attn_output.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_attn_output  = hf_roberta.RobertaSelfAttention(hf_config)\\nhf_attn_output.load_state_dict(state, strict=True)\\n\\nmy_output = my_attn_output(x_embed, mask, key=key)\\nhf_output = hf_attn_output(x_embed_torch, mask_torch)\\n\\ncheck(my_output.array, hf_output[0].detach(), ppp=True)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaSelfAttention\n",
    "\n",
    "my_attn_output = my_roberta.RobertaSelfAttention.init(my_config, key=key)\n",
    "state = my_attn_output.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_attn_output  = hf_roberta.RobertaSelfAttention(hf_config)\n",
    "hf_attn_output.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_attn_output(x_embed, mask, key=key)\n",
    "hf_output = hf_attn_output(x_embed_torch, mask_torch)\n",
    "\n",
    "check(my_output.array, hf_output[0].detach(), ppp=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaAttention\\n\\nmy_func = my_roberta.RobertaAttention.init(my_config, key=key)\\nstate = my_func.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_func = hf_roberta.RobertaAttention(hf_config)\\nhf_func.load_state_dict(state, strict=True)\\n\\nmy_output = my_func(hidden_states=x_embed, attention_mask=mask, key=key)\\nhf_output = hf_func(hidden_states=x_embed_torch, attention_mask=mask_torch)\\n\\ncheck(my_output.array, hf_output[0].detach(), ppp=True)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaAttention\n",
    "\n",
    "my_func = my_roberta.RobertaAttention.init(my_config, key=key)\n",
    "state = my_func.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_func = hf_roberta.RobertaAttention(hf_config)\n",
    "hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_func(hidden_states=x_embed, attention_mask=mask, key=key)\n",
    "hf_output = hf_func(hidden_states=x_embed_torch, attention_mask=mask_torch)\n",
    "\n",
    "check(my_output.array, hf_output[0].detach(), ppp=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaIntermediate\\n\\nmy_int_output = my_roberta.RobertaIntermediate.init(my_config, key=key)\\nstate = my_int_output.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_int_output = hf_roberta.RobertaIntermediate(hf_config)\\nhf_int_output.load_state_dict(state, strict=True)\\n\\nmy_output = my_int_output(x_embed, key=key)\\nhf_output = hf_int_output(x_embed_torch)\\n\\ncheck(my_output.array, hf_output.detach(), ppp=True)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaIntermediate\n",
    "\n",
    "my_int_output = my_roberta.RobertaIntermediate.init(my_config, key=key)\n",
    "state = my_int_output.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_int_output = hf_roberta.RobertaIntermediate(hf_config)\n",
    "hf_int_output.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_int_output(x_embed, key=key)\n",
    "hf_output = hf_int_output(x_embed_torch)\n",
    "\n",
    "check(my_output.array, hf_output.detach(), ppp=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaOutput\\n\\nmy_int_output = my_roberta.RobertaOutput.init(my_config, key=key)\\nstate = my_int_output.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_int_output = hf_roberta.RobertaOutput(hf_config)\\nhf_int_output.load_state_dict(state, strict=True)\\n\\nmy_output = my_int_output(x_mlp, x_embed, key=key)\\nhf_output = hf_int_output(x_mlp_torch, x_embed_torch)\\n\\ncheck(my_output.array, hf_output.detach(), ppp=True)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaOutput\n",
    "\n",
    "my_int_output = my_roberta.RobertaOutput.init(my_config, key=key)\n",
    "state = my_int_output.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_int_output = hf_roberta.RobertaOutput(hf_config)\n",
    "hf_int_output.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_int_output(x_mlp, x_embed, key=key)\n",
    "hf_output = hf_int_output(x_mlp_torch, x_embed_torch)\n",
    "\n",
    "check(my_output.array, hf_output.detach(), ppp=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaLayer\\n\\nmy_func = my_roberta.RobertaLayer.init(my_config, key=key)\\nstate = my_func.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_func = hf_roberta.RobertaLayer(hf_config)\\nhf_func.load_state_dict(state, strict=True)\\n\\nmy_output = my_func(hidden_states=x_embed, attention_mask=mask, key=key)\\nhf_output = hf_func(hidden_states=x_embed_torch, attention_mask=mask_torch)\\n\\ncheck(my_output.array, hf_output[0].detach(), False, False, True, 1e-4)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaLayer\n",
    "\n",
    "my_func = my_roberta.RobertaLayer.init(my_config, key=key)\n",
    "state = my_func.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_func = hf_roberta.RobertaLayer(hf_config)\n",
    "hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_func(hidden_states=x_embed, attention_mask=mask, key=key)\n",
    "hf_output = hf_func(hidden_states=x_embed_torch, attention_mask=mask_torch)\n",
    "\n",
    "check(my_output.array, hf_output[0].detach(), False, False, True, 1e-4)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaEncoder\\n\\nmy_func = my_roberta.RobertaEncoder.init(my_config, key=key)\\nstate = my_func.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_func = hf_roberta.RobertaEncoder(hf_config)\\nhf_func.load_state_dict(state, strict=True)\\n\\nmy_output = my_func(hidden_states=x_embed, attention_mask=mask, key=key)\\nhf_output = hf_func(hidden_states=x_embed_torch, attention_mask=mask_torch)\\n\\ncheck(my_output.array, hf_output[0].detach(), False, False, True, 1e-4)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaEncoder\n",
    "\n",
    "my_func = my_roberta.RobertaEncoder.init(my_config, key=key)\n",
    "state = my_func.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_func = hf_roberta.RobertaEncoder(hf_config)\n",
    "hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_func(hidden_states=x_embed, attention_mask=mask, key=key)\n",
    "hf_output = hf_func(hidden_states=x_embed_torch, attention_mask=mask_torch)\n",
    "\n",
    "check(my_output.array, hf_output[0].detach(), False, False, True, 1e-4)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaEmbedding\\n\\nVocab = hax.Axis(\"vocab\", my_config.vocab_size)\\n\\ninput_embeds = hax.random.uniform(key, (Batch, Pos, Embed))\\ninput_embeds = input_embeds[{\"position\": slice(0,-2)}]\\ninput_ids = hax.random.randint(key, (Batch, Pos), minval = 0, maxval = my_config.vocab_size)\\ninput_ids = input_ids[{\"position\": slice(0,-2)}]\\n\\ninput_embeds_torch = torch.from_numpy(np.array(input_embeds.array))\\ninput_ids_torch = torch.from_numpy(np.array(input_ids.array))\\n\\nmy_func = my_roberta.RobertaEmbedding.init(Vocab, my_config, key=key)\\nstate = my_func.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_func = hf_roberta.RobertaEmbeddings(hf_config)\\nhf_func.load_state_dict(state, strict=True)\\n\\n# my_output = my_func.embed(input_embeds=input_embeds, key=key)\\n# hf_output = hf_func(inputs_embeds=input_embeds_torch)\\n\\nmy_output = my_func.embed(input_ids=input_ids, key=key)\\nhf_output = hf_func(input_ids=input_ids_torch)\\n\\ncheck(my_output.array, hf_output.detach())'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaEmbedding\n",
    "\n",
    "Vocab = hax.Axis(\"vocab\", my_config.vocab_size)\n",
    "\n",
    "input_embeds = hax.random.uniform(key, (Batch, Pos, Embed))\n",
    "input_embeds = input_embeds[{\"position\": slice(0,-2)}]\n",
    "input_ids = hax.random.randint(key, (Batch, Pos), minval = 0, maxval = my_config.vocab_size)\n",
    "input_ids = input_ids[{\"position\": slice(0,-2)}]\n",
    "\n",
    "input_embeds_torch = torch.from_numpy(np.array(input_embeds.array))\n",
    "input_ids_torch = torch.from_numpy(np.array(input_ids.array))\n",
    "\n",
    "my_func = my_roberta.RobertaEmbedding.init(Vocab, my_config, key=key)\n",
    "state = my_func.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_func = hf_roberta.RobertaEmbeddings(hf_config)\n",
    "hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "# my_output = my_func.embed(input_embeds=input_embeds, key=key)\n",
    "# hf_output = hf_func(inputs_embeds=input_embeds_torch)\n",
    "\n",
    "my_output = my_func.embed(input_ids=input_ids, key=key)\n",
    "hf_output = hf_func(input_ids=input_ids_torch)\n",
    "\n",
    "check(my_output.array, hf_output.detach())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaPooler\\n\\nmy_pool = my_roberta.RobertaPooler.init(my_config, key=key)\\nstate = my_pool.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_pool = hf_roberta.RobertaPooler(hf_config)\\nhf_pool.load_state_dict(state, strict=True)\\n\\nmy_output = my_pool(x_embed, key=key)\\nhf_output = hf_pool(x_embed_torch)\\n\\ncheck(my_output.array, hf_output.detach(), ppp=True)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaPooler\n",
    "\n",
    "my_pool = my_roberta.RobertaPooler.init(my_config, key=key)\n",
    "state = my_pool.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_pool = hf_roberta.RobertaPooler(hf_config)\n",
    "hf_pool.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_pool(x_embed, key=key)\n",
    "hf_output = hf_pool(x_embed_torch)\n",
    "\n",
    "check(my_output.array, hf_output.detach(), ppp=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Testing RobertaModel\\n\\nmy_pool = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=True, key=key)\\nstate = my_pool.to_state_dict()\\n\\nstate = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\\n\\nprint(state.keys())\\n\\nhf_pool = hf_roberta.RobertaModel(hf_config, add_pooling_layer=True)\\nhf_pool.load_state_dict(state, strict=True)\\n\\nmy_output = my_pool(input_ids = input_ids, attention_mask=mask, key=key)\\nhf_output = hf_pool(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\\n\\ncheck(my_output[0].array, hf_output[0].detach(), ppp=True)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Testing RobertaModel\n",
    "\n",
    "my_pool = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=True, key=key)\n",
    "state = my_pool.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_pool = hf_roberta.RobertaModel(hf_config, add_pooling_layer=True)\n",
    "hf_pool.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_pool(input_ids = input_ids, attention_mask=mask, key=key)\n",
    "hf_output = hf_pool(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "check(my_output[0].array, hf_output[0].detach(), ppp=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias'])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for RobertaForMaskedLM:\n\tMissing key(s) in state_dict: \"lm_head.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(state\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     10\u001b[0m hf_pool \u001b[38;5;241m=\u001b[39m hf_roberta\u001b[38;5;241m.\u001b[39mRobertaForMaskedLM(hf_config)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mhf_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m my_output \u001b[38;5;241m=\u001b[39m my_pool(input_ids \u001b[38;5;241m=\u001b[39m input_ids, attention_mask\u001b[38;5;241m=\u001b[39mmask, key\u001b[38;5;241m=\u001b[39mkey)\n\u001b[0;32m     14\u001b[0m hf_output \u001b[38;5;241m=\u001b[39m hf_pool(input_ids \u001b[38;5;241m=\u001b[39m input_ids_torch, attention_mask\u001b[38;5;241m=\u001b[39mmask_torch, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\torch\\nn\\modules\\module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for RobertaForMaskedLM:\n\tMissing key(s) in state_dict: \"lm_head.bias\". "
     ]
    }
   ],
   "source": [
    "# Testing RobertaForMaskedLM\n",
    "\n",
    "my_pool = my_roberta.RobertaForMaskedLM.init(Vocab, my_config, key=key)\n",
    "state = my_pool.to_state_dict()\n",
    "\n",
    "state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "print(state.keys())\n",
    "\n",
    "hf_pool = hf_roberta.RobertaForMaskedLM(hf_config)\n",
    "hf_pool.load_state_dict(state, strict=True)\n",
    "\n",
    "my_output = my_pool(input_ids = input_ids, attention_mask=mask, key=key)\n",
    "hf_output = hf_pool(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "check(my_output[0].array, hf_output[0].detach(), ppp=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
