{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julie\\anaconda3\\envs\\levanter2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-04 10:40:36,597\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import roberta as my_roberta\n",
    "from transformers.models.roberta import modeling_roberta as hf_roberta\n",
    "\n",
    "import torch\n",
    "import haliax as hax\n",
    "import jax.random as jrandom\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from time import time\n",
    "\n",
    "hf_model_str = \"FacebookAI/roberta-base\"\n",
    "\n",
    "hf_config = AutoConfig.from_pretrained(hf_model_str)\n",
    "hf_config.hidden_dropout_prob = 0\n",
    "hf_config.attention_probs_dropout_prob = 0\n",
    "hf_config.pad_token_id = -1\n",
    "my_config = my_roberta.RobertaConfig.from_hf_config(hf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 1725471642\n"
     ]
    }
   ],
   "source": [
    "seed = time()\n",
    "print(f\"seed: {int(seed)}\")\n",
    "key_vars = jrandom.PRNGKey(int(seed))\n",
    "\n",
    "EmbedAtt = my_config.EmbedAtt\n",
    "Embed = my_config.Embed\n",
    "Mlp = my_config.Mlp\n",
    "Pos = my_config.Pos\n",
    "KeyPos = my_config.KeyPos\n",
    "Heads = my_config.Heads\n",
    "\n",
    "cut_end_for_bounds = False \n",
    "\n",
    "Batch = hax.Axis(\"batch\", 2)\n",
    "Vocab = hax.Axis(\"vocab\", my_config.vocab_size)\n",
    "\n",
    "keys = jrandom.split(key_vars, 6)\n",
    "\n",
    "input_ids = hax.random.randint(keys[0], (Batch, Pos), minval = 3, maxval = my_config.vocab_size)\n",
    "\n",
    "if cut_end_for_bounds:\n",
    "    input_ids = input_ids[{\"position\": slice(0,-2)}]\n",
    "\n",
    "input_ids_torch = torch.from_numpy(np.array(input_ids.array))\n",
    "input_embeds = hax.random.normal(keys[1], (Batch, Pos, Embed))\n",
    "\n",
    "if cut_end_for_bounds:\n",
    "    input_embeds = input_embeds[{\"position\": slice(0,-2)}]\n",
    "\n",
    "input_embeds_torch = torch.from_numpy(np.array(input_embeds.array))\n",
    "\n",
    "# mask = hax.random.randint(keys[2], (Batch, Pos), minval = 0, maxval = 2)\n",
    "mask = hax.ones((Batch, Pos))\n",
    "\n",
    "if cut_end_for_bounds:\n",
    "    mask = mask[{\"position\": slice(0,-2)}]\n",
    "\n",
    "mask_torch = torch.from_numpy(np.array(mask.array))\n",
    "\n",
    "mask_materialized = hax.ones((Batch, Heads, Pos, KeyPos)) * -jnp.inf\n",
    "\n",
    "if cut_end_for_bounds:\n",
    "    mask_materialized = mask_materialized[{\"position\": slice(0,-2), \"key_position\": slice(0,-2)}]\n",
    "\n",
    "mask_materialized_torch = torch.from_numpy(np.array(mask_materialized.array))\n",
    "\n",
    "features = input_embeds[{\"position\": 0}]\n",
    "features_torch = torch.from_numpy(np.array(features.array))\n",
    "\n",
    "x_embed_att = input_embeds.rename({\"embed\": \"embed_att\"})\n",
    "\n",
    "if cut_end_for_bounds:\n",
    "    x_embed_att = x_embed_att[{\"position\": slice(0,-2)}]\n",
    "\n",
    "x_embed_att_torch = torch.from_numpy(np.array(x_embed_att.array))\n",
    "x_mlp = hax.random.normal(keys[5], (Batch, Pos, Mlp))\n",
    "\n",
    "if cut_end_for_bounds:\n",
    "    x_mlp = x_mlp[{\"position\": slice(0,-2)}]\n",
    "    \n",
    "x_mlp_torch = torch.from_numpy(np.array(x_mlp.array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(my_output, hf_output, precision=1e-4):\n",
    "    \n",
    "    assert (np.array(my_output.shape) == np.array(hf_output.shape)).all()\n",
    "    # print(my_output.shape)\n",
    "    # print(hf_output.shape)\n",
    "\n",
    "    acc = np.isclose(hf_output, np.array(my_output), rtol=precision, atol=precision).mean()\n",
    "\n",
    "    # stats = (torch.tensor(np.array(my_output)).abs().mean(), torch.tensor(np.array(hf_output)).abs().mean()) \n",
    "    stats = (torch.linalg.norm(torch.tensor(np.array(my_output))), torch.linalg.norm(torch.tensor(np.array(hf_output))))\n",
    "    \n",
    "    difference = torch.tensor(np.array(my_output)) - torch.tensor(np.array(hf_output))\n",
    "\n",
    "    diffs = difference.abs().mean()\n",
    "\n",
    "    to_print = f\"acc: {acc} \\t norms: {stats} \\t diffs: {diffs}\"\n",
    "    \n",
    "    return acc, stats, diffs, to_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing RobertaSelfOutput\n",
    "\n",
    "# def test_RobertaSelfOutput(key):\n",
    "#     k_1, k_2 = jrandom.split(key, 2)\n",
    "#     my_func = my_roberta.RobertaSelfOutput.init(my_config, key=k_1)\n",
    "#     state = my_func.to_state_dict()\n",
    "\n",
    "#     state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "#     # # print(state.keys())\n",
    "\n",
    "#     hf_func = hf_roberta.RobertaSelfOutput(hf_config)\n",
    "#     hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "#     my_output = my_func(x_embed_att, input_embeds, key=k_2)\n",
    "#     hf_output = hf_func(x_embed_att_torch, input_embeds_torch)\n",
    "\n",
    "#     return check(my_output.array, hf_output.detach())\n",
    "\n",
    "# # Testing RobertaSelfAttention\n",
    "\n",
    "# def test_RobertaSelfAttention(key):\n",
    "#     k_1, k_2 = jrandom.split(key, 2)\n",
    "\n",
    "#     my_func = my_roberta.RobertaSelfAttention.init(my_config, key=k_1)\n",
    "#     state = my_func.to_state_dict()\n",
    "\n",
    "#     state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "#     # print(state.keys())\n",
    "\n",
    "#     hf_func  = hf_roberta.RobertaSelfAttention(hf_config)\n",
    "#     hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "#     my_output = my_func(input_embeds, mask, key=k_2)\n",
    "#     hf_output = hf_func(input_embeds_torch, mask_torch_materialized)\n",
    "\n",
    "#     return check(my_output.array, hf_output[0].detach())\n",
    "\n",
    "# # Testing RobertaAttention\n",
    "\n",
    "# def test_RobertaAttention(key):\n",
    "#     k_1, k_2 = jrandom.split(key, 2)\n",
    "#     my_func = my_roberta.RobertaAttention.init(my_config, key=k_1)\n",
    "#     state = my_func.to_state_dict()\n",
    "\n",
    "#     state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "#     # print(state.keys())\n",
    "\n",
    "#     hf_func = hf_roberta.RobertaAttention(hf_config)\n",
    "#     hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "#     my_output = my_func(hidden_states=input_embeds, attention_mask=mask, key=k_2)\n",
    "#     hf_output = hf_func(hidden_states=input_embeds_torch, attention_mask=mask_torch_materialized)\n",
    "\n",
    "#     return check(my_output.array, hf_output[0].detach())\n",
    "\n",
    "# # Testing RobertaIntermediate\n",
    "\n",
    "# def test_RobertaIntermediate(key):\n",
    "#     k_1, k_2 = jrandom.split(key, 2)\n",
    "#     my_func = my_roberta.RobertaIntermediate.init(my_config, key=k_1)\n",
    "#     state = my_func.to_state_dict()\n",
    "\n",
    "#     state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "#     # print(state.keys())\n",
    "\n",
    "#     hf_func = hf_roberta.RobertaIntermediate(hf_config)\n",
    "#     hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "#     my_output = my_func(input_embeds, key=k_2)\n",
    "#     hf_output = hf_func(input_embeds_torch)\n",
    "\n",
    "#     return check(my_output.array, hf_output.detach())\n",
    "\n",
    "# # Testing RobertaOutput\n",
    "\n",
    "# def test_RobertaOutput(key):\n",
    "#     k_1, k_2 = jrandom.split(key, 2)\n",
    "\n",
    "#     my_func = my_roberta.RobertaOutput.init(my_config, key=k_1)\n",
    "#     state = my_func.to_state_dict()\n",
    "\n",
    "#     state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "#     # print(state.keys())\n",
    "\n",
    "#     hf_func = hf_roberta.RobertaOutput(hf_config)\n",
    "#     hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "#     my_output = my_func(x_mlp, input_embeds, key=k_2)\n",
    "#     hf_output = hf_func(x_mlp_torch, input_embeds_torch)\n",
    "\n",
    "#     return check(my_output.array, hf_output.detach())\n",
    "\n",
    "# # Testing RobertaLayer\n",
    "\n",
    "# def test_RobertaLayer(key):\n",
    "#     k_1, k_2 = jrandom.split(key, 2)\n",
    "#     my_func = my_roberta.RobertaLayer.init(my_config, key=k_1)\n",
    "#     state = my_func.to_state_dict()\n",
    "\n",
    "#     state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "#     # print(state.keys())\n",
    "\n",
    "#     hf_func = hf_roberta.RobertaLayer(hf_config)\n",
    "#     hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "#     my_output = my_func(hidden_states=input_embeds, attention_mask=mask, key=k_2)\n",
    "#     hf_output = hf_func(hidden_states=input_embeds_torch, attention_mask=mask_torch_materialized)\n",
    "\n",
    "#     return check(my_output.array, hf_output[0].detach())\n",
    "\n",
    "# # Testing RobertaEncoder\n",
    "\n",
    "# def test_RobertaEncoder(key):\n",
    "#     k_1, k_2 = jrandom.split(key, 2)\n",
    "#     my_func = my_roberta.RobertaEncoder.init(my_config, key=k_1)\n",
    "#     state = my_func.to_state_dict()\n",
    "\n",
    "#     state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "#     # print(state.keys())\n",
    "\n",
    "#     hf_func = hf_roberta.RobertaEncoder(hf_config)\n",
    "#     hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "#     my_output = my_func(hidden_states=input_embeds, attention_mask=mask, key=k_2)\n",
    "#     hf_output = hf_func(hidden_states=input_embeds_torch, attention_mask=mask_torch_materialized)\n",
    "\n",
    "#     return check(my_output.array, hf_output[0].detach())\n",
    "\n",
    "# # Testing RobertaEmbedding\n",
    "\n",
    "# def test_RobertaEmbedding(key, ids = True):\n",
    "#     k_1, k_2 = jrandom.split(key, 2)\n",
    "#     my_func = my_roberta.RobertaEmbedding.init(Vocab, my_config, key=k_1)\n",
    "#     state = my_func.to_state_dict()\n",
    "\n",
    "#     state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "#     # print(state.keys())\n",
    "\n",
    "#     hf_func = hf_roberta.RobertaEmbeddings(hf_config)\n",
    "#     hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "#     if ids:\n",
    "#         my_output = my_func.embed(input_ids=input_ids, key=k_2)\n",
    "#         hf_output = hf_func(input_ids=input_ids_torch)\n",
    "#     else:        \n",
    "#         my_output = my_func.embed(input_embeds=input_embeds, key=k_2)\n",
    "#         hf_output = hf_func(inputs_embeds=input_embeds_torch)\n",
    "\n",
    "#     return check(my_output.array, hf_output.detach())\n",
    "\n",
    "# # Testing RobertaPooler\n",
    "\n",
    "# def test_RobertaPooler(key):\n",
    "#     k_1, k_2 = jrandom.split(key, 2)\n",
    "#     my_func = my_roberta.RobertaPooler.init(my_config, key=k_1)\n",
    "#     state = my_func.to_state_dict()\n",
    "\n",
    "#     state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "#     # print(state.keys())\n",
    "\n",
    "#     hf_func = hf_roberta.RobertaPooler(hf_config)\n",
    "#     hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "#     my_output = my_func(input_embeds, key=k_2)\n",
    "#     hf_output = hf_func(input_embeds_torch)\n",
    "\n",
    "#     return check(my_output.array, hf_output.detach())\n",
    "\n",
    "# # Testing RobertaModel\n",
    "\n",
    "# def test_RobertaModel(key, ids = True, pool = True):\n",
    "#     k_1, k_2 = jrandom.split(key, 2)\n",
    "#     my_func = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=pool, key=k_1)\n",
    "#     state = my_func.to_state_dict()\n",
    "\n",
    "#     state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "#     # print(state.keys())\n",
    "\n",
    "#     hf_func = hf_roberta.RobertaModel(hf_config, add_pooling_layer=pool)\n",
    "#     hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "#     if ids:\n",
    "#         my_output = my_func(input_ids = input_ids, attention_mask=mask, key=k_2)\n",
    "#         hf_output = hf_func(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "#     else:\n",
    "#         my_output = my_func(input_embeds = input_embeds, attention_mask=mask, key=k_2)\n",
    "#         hf_output = hf_func(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "#     if pool:\n",
    "#         return check(my_output[1].array, hf_output[1].detach())\n",
    "#     else:\n",
    "#         return check(my_output[0].array, hf_output[0].detach())\n",
    "\n",
    "# # Testing RobertaLMHead\n",
    "\n",
    "# def test_RobertaLMHead(key):\n",
    "#     k_1, k_2 = jrandom.split(key, 2)\n",
    "#     my_func = my_roberta.RobertaLMHead.init(Vocab, my_config, key=k_1)\n",
    "#     state = my_func.to_state_dict()\n",
    "\n",
    "#     state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "#     state[\"bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "\n",
    "#     # print(state.keys())\n",
    "\n",
    "#     hf_func = hf_roberta.RobertaLMHead(hf_config)\n",
    "#     hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "#     my_output = my_func(features, key=k_2)\n",
    "#     hf_output = hf_func(features_torch)\n",
    "\n",
    "#     return check(my_output.array, hf_output.detach())\n",
    "\n",
    "# # Testing RobertaForMaskedLM\n",
    "\n",
    "# def test_RobertaForMaskedLM(key, ids = True):\n",
    "#     k_1, k_2 = jrandom.split(key, 2)\n",
    "#     my_pool = my_roberta.RobertaForMaskedLM.init(Vocab, my_config, key=k_1)\n",
    "#     state = my_pool.to_state_dict()\n",
    "\n",
    "#     state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "#     state[\"lm_head.bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "\n",
    "#     # print(state.keys())\n",
    "\n",
    "#     hf_pool = hf_roberta.RobertaForMaskedLM(hf_config)\n",
    "#     hf_pool.load_state_dict(state, strict=True)\n",
    "\n",
    "#     if ids:\n",
    "#         my_output = my_pool(input_ids = input_ids, attention_mask=mask, key=k_2)\n",
    "#         hf_output = hf_pool(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "#     else:\n",
    "#         my_output = my_pool(input_embeds = input_embeds, attention_mask=mask, key=k_2)\n",
    "#         hf_output = hf_pool(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "#     return check(my_output.array, hf_output[0].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = time()\n",
    "# print(f\"seed: {int(seed)}\")\n",
    "# key_vars = jrandom.PRNGKey(int(seed))\n",
    "# keys = jrandom.split(key_vars, 15)\n",
    "\n",
    "# print(f\"test_RobertaSelfOutput: {out_func(test_RobertaSelfOutput(keys[0]))}\")\n",
    "# print(f\"test_RobertaSelfAttention: {out_func(test_RobertaSelfAttention(keys[1]))}\")\n",
    "# print(f\"test_RobertaAttention: {out_func(test_RobertaAttention(keys[2]))}\")\n",
    "# print(f\"test_RobertaIntermediate: {out_func(test_RobertaIntermediate(keys[3]))}\")\n",
    "# print(f\"test_RobertaOutput: {out_func(test_RobertaOutput(keys[4]))}\")\n",
    "# print(f\"test_RobertaEmbedding(ids = True): {out_func(test_RobertaEmbedding(keys[7], ids = True))}\")\n",
    "# print(f\"test_RobertaEmbedding(ids = False): {out_func(test_RobertaEmbedding(keys[8], ids = False))}\")\n",
    "# print(f\"test_RobertaModel(ids = True, pool = True): {out_func(test_RobertaModel(keys[9], ids = True, pool = True))}\")\n",
    "# print(f\"test_RobertaModel(ids = False, pool = False): {out_func(test_RobertaModel(keys[10], ids = False, pool = False))}\")\n",
    "# print(f\"test_RobertaModel(ids = True, pool = True): {out_func(test_RobertaModel(keys[9], ids = True, pool = True))}\")\n",
    "# print(f\"test_RobertaModel(ids = False, pool = False): {out_func(test_RobertaModel(keys[10], ids = False, pool = False))}\")\n",
    "# print(f\"test_RobertaPooler: {out_func(test_RobertaPooler(keys[11]))}\")\n",
    "# print(f\"test_RobertaLMHead: {out_func(test_RobertaLMHead(keys[12]))}\")\n",
    "# print(f\"test_RobertaForMaskedLM(ids = True): {out_func(test_RobertaForMaskedLM(keys[13], ids = True))}\")\n",
    "# print(f\"test_RobertaForMaskedLM(ids = False): {out_func(test_RobertaForMaskedLM(keys[14], ids = False))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 1725471643\n"
     ]
    }
   ],
   "source": [
    "seed = time()\n",
    "print(f\"seed: {int(seed)}\")\n",
    "key = jrandom.PRNGKey(int(seed))\n",
    "\n",
    "def test_RobertaModel_Output(key, ids = False, pool = True):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=pool, output_hidden_states=True, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "\n",
    "    # print(state.keys())\n",
    "\n",
    "    hf_func = hf_roberta.RobertaModel(hf_config, add_pooling_layer=pool)\n",
    "    hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "    if ids:\n",
    "        my_output = my_func(input_ids = input_ids, attention_mask=mask, key=k_2)\n",
    "        hf_output = hf_func(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False, output_hidden_states=True)\n",
    "    else:\n",
    "        my_output = my_func(input_embeds = input_embeds, attention_mask=mask, key=k_2)\n",
    "        hf_output = hf_func(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False, output_hidden_states=True)\n",
    "\n",
    "    return my_output, hf_output\n",
    "\n",
    "my_output_ids, hf_output_ids = test_RobertaModel_Output(key, ids=True)\n",
    "my_output_embeds, hf_output_embeds = test_RobertaModel_Output(key, ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_out: acc: 1.0 \t norms: (tensor(888.5305), tensor(888.5305)) \t diffs: 3.735790983228071e-07\n",
      "pool_out: acc: 1.0 \t norms: (tensor(24.1699), tensor(24.1699)) \t diffs: 2.6738598535303026e-07\n",
      "intermediates:\n",
      "acc: 1.0 \t norms: (tensor(888.5306), tensor(888.5306)) \t diffs: 1.842970362986307e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5308), tensor(888.5308)) \t diffs: 2.625348543006112e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5301), tensor(888.5301)) \t diffs: 3.2068956556940975e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5295), tensor(888.5295)) \t diffs: 3.396997101390298e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5296), tensor(888.5297)) \t diffs: 3.419580139052414e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5310), tensor(888.5310)) \t diffs: 3.721844734627666e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5298), tensor(888.5299)) \t diffs: 3.591211736875266e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5299), tensor(888.5299)) \t diffs: 3.513960677992145e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5300), tensor(888.5300)) \t diffs: 3.739319538453856e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5305), tensor(888.5305)) \t diffs: 3.6201470265950775e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5309), tensor(888.5309)) \t diffs: 3.658180958154844e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5305), tensor(888.5305)) \t diffs: 3.735790983228071e-07\n"
     ]
    }
   ],
   "source": [
    "my_out, hf_out = my_output_ids[0], hf_output_ids[0]\n",
    "\n",
    "print(f\"model_out: {check(my_out.array, hf_out.detach())[3]}\")\n",
    "\n",
    "my_pool, hf_pool = my_output_ids[1], hf_output_ids[1]\n",
    "\n",
    "print(f\"pool_out: {check(my_pool.array, hf_pool.detach())[3]}\")\n",
    "\n",
    "print(\"intermediates:\")\n",
    "my_ints, hf_ints = my_output_ids[2], hf_output_ids[2][1:]\n",
    "\n",
    "for i,j in zip(my_ints, hf_ints):\n",
    "    print(check(i.array,j.detach())[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_out: acc: 1.0 \t norms: (tensor(888.5308), tensor(888.5308)) \t diffs: 3.864420250465628e-07\n",
      "pool_out: acc: 1.0 \t norms: (tensor(24.0400), tensor(24.0400)) \t diffs: 2.5442224682592496e-07\n",
      "intermediates:\n",
      "acc: 1.0 \t norms: (tensor(888.5295), tensor(888.5295)) \t diffs: 1.46142554058315e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5310), tensor(888.5311)) \t diffs: 2.35209114407553e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5301), tensor(888.5302)) \t diffs: 3.0417106700042496e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5303), tensor(888.5303)) \t diffs: 3.522779365994211e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5314), tensor(888.5314)) \t diffs: 3.7978762179591286e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5302), tensor(888.5302)) \t diffs: 3.9330373624579806e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5305), tensor(888.5304)) \t diffs: 3.8590263784499257e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5306), tensor(888.5306)) \t diffs: 3.735180200692412e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5292), tensor(888.5291)) \t diffs: 3.75227983795412e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5312), tensor(888.5312)) \t diffs: 3.7935546970402356e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5292), tensor(888.5292)) \t diffs: 3.81289993356404e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5308), tensor(888.5308)) \t diffs: 3.864420250465628e-07\n"
     ]
    }
   ],
   "source": [
    "my_out, hf_out = my_output_embeds[0], hf_output_embeds[0]\n",
    "\n",
    "print(f\"model_out: {check(my_out.array, hf_out.detach())[3]}\")\n",
    "\n",
    "my_pool, hf_pool = my_output_embeds[1], hf_output_embeds[1]\n",
    "\n",
    "print(f\"pool_out: {check(my_pool.array, hf_pool.detach())[3]}\")\n",
    "\n",
    "print(\"intermediates:\")\n",
    "my_ints, hf_ints = my_output_embeds[2], hf_output_embeds[2][1:]\n",
    "\n",
    "for i,j in zip(my_ints, hf_ints):\n",
    "    print(check(i.array,j.detach())[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 1725471659\n"
     ]
    }
   ],
   "source": [
    "seed = time()\n",
    "print(f\"seed: {int(seed)}\")\n",
    "key = jrandom.PRNGKey(int(seed))\n",
    "\n",
    "def test_RobertaForMaskedLM_Output(key, ids = True):\n",
    "    k_1, k_2 = jrandom.split(key, 2)\n",
    "    my_func = my_roberta.RobertaForMaskedLM.init(Vocab, my_config, output_hidden_states=True, key=k_1)\n",
    "    state = my_func.to_state_dict()\n",
    "\n",
    "    state = {k: torch.from_numpy(np.array(v)) for k, v in state.items()}\n",
    "    \n",
    "    state[\"lm_head.bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "\n",
    "    hf_func = hf_roberta.RobertaForMaskedLM(hf_config)\n",
    "    hf_func.load_state_dict(state, strict=True)\n",
    "\n",
    "    if ids:\n",
    "        my_output = my_func(input_ids = input_ids, attention_mask=mask, key=k_2)\n",
    "        hf_output = hf_func(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False, output_hidden_states=True)\n",
    "    else:\n",
    "        my_output = my_func(input_embeds = input_embeds, attention_mask=mask, key=k_2)\n",
    "        hf_output = hf_func(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False, output_hidden_states=True)\n",
    "\n",
    "    return my_output, hf_output\n",
    "\n",
    "my_mlm_output_ids, hf_mlm_output_ids = test_RobertaForMaskedLM_Output(key, ids=True)\n",
    "my_mlm_output_embeds, hf_mlm_output_embeds = test_RobertaForMaskedLM_Output(key, ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlm_out: acc: 0.001719795589213743 \t norms: (tensor(7055.4185), tensor(7059.8276)) \t diffs: 0.06642061471939087\n",
      "intermediates:\n",
      "acc: 0.019604713845654993 \t norms: (tensor(888.5299), tensor(888.5306)) \t diffs: 0.5695138573646545\n",
      "acc: 0.024172138456549936 \t norms: (tensor(888.5310), tensor(888.5300)) \t diffs: 0.46615326404571533\n",
      "acc: 0.030564759646562904 \t norms: (tensor(888.5312), tensor(888.5323)) \t diffs: 0.37349948287010193\n",
      "acc: 0.04000106395914397 \t norms: (tensor(888.5294), tensor(888.5300)) \t diffs: 0.28456440567970276\n",
      "acc: 0.05416818660830091 \t norms: (tensor(888.5305), tensor(888.5297)) \t diffs: 0.21144814789295197\n",
      "acc: 0.07160065053501946 \t norms: (tensor(888.5291), tensor(888.5300)) \t diffs: 0.16202779114246368\n",
      "acc: 0.08982095087548637 \t norms: (tensor(888.5302), tensor(888.5295)) \t diffs: 0.12657980620861053\n",
      "acc: 0.11758268482490272 \t norms: (tensor(888.5289), tensor(888.5308)) \t diffs: 0.09682736545801163\n",
      "acc: 0.1463805123216602 \t norms: (tensor(888.5303), tensor(888.5312)) \t diffs: 0.07892335206270218\n",
      "acc: 0.16748110205901426 \t norms: (tensor(888.5292), tensor(888.5310)) \t diffs: 0.06763624399900436\n",
      "acc: 0.18012701645590143 \t norms: (tensor(888.5292), tensor(888.5284)) \t diffs: 0.062167659401893616\n",
      "acc: 0.17417264510376135 \t norms: (tensor(888.5310), tensor(888.5300)) \t diffs: 0.06321203708648682\n"
     ]
    }
   ],
   "source": [
    "my_out, hf_out = my_mlm_output_ids[0], hf_mlm_output_ids[0]\n",
    "\n",
    "print(f\"mlm_out: {check(my_out.array, hf_out.detach())[3]}\")\n",
    "\n",
    "print(\"intermediates:\")\n",
    "my_ints, hf_ints = my_mlm_output_ids[1], hf_mlm_output_ids[1][1:]\n",
    "\n",
    "for i,j in zip(my_ints, hf_ints):\n",
    "    print(check(i.array,j.detach(), precision = 0.01)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlm_out: acc: 1.0 \t norms: (tensor(7033.6245), tensor(7033.6240)) \t diffs: 5.174669013285893e-07\n",
      "intermediates:\n",
      "acc: 1.0 \t norms: (tensor(888.5309), tensor(888.5310)) \t diffs: 1.4689783256471856e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5306), tensor(888.5306)) \t diffs: 2.3626945733212779e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5312), tensor(888.5312)) \t diffs: 3.0318486210489937e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5305), tensor(888.5305)) \t diffs: 3.531020809077745e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5297), tensor(888.5297)) \t diffs: 3.7493518334486e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5297), tensor(888.5297)) \t diffs: 3.8230905374803115e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5295), tensor(888.5296)) \t diffs: 3.8595226214965805e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5308), tensor(888.5308)) \t diffs: 3.713914793479489e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5317), tensor(888.5318)) \t diffs: 3.5173252399545163e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5316), tensor(888.5316)) \t diffs: 3.4720503094831656e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5290), tensor(888.5290)) \t diffs: 3.541817932273261e-07\n",
      "acc: 1.0 \t norms: (tensor(888.5295), tensor(888.5295)) \t diffs: 3.6307170603322447e-07\n"
     ]
    }
   ],
   "source": [
    "my_out, hf_out = my_mlm_output_embeds[0], hf_mlm_output_embeds[0]\n",
    "\n",
    "print(f\"mlm_out: {check(my_out.array, hf_out.detach())[3]}\")\n",
    "\n",
    "print(\"intermediates:\")\n",
    "my_ints, hf_ints = my_mlm_output_embeds[1], hf_mlm_output_embeds[1][1:]\n",
    "\n",
    "for i,j in zip(my_ints, hf_ints):\n",
    "    print(check(i.array,j.detach())[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias'])\n",
      "dict_keys(['encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias'])\n",
      "dict_keys(['dense.weight', 'dense.bias', 'layer_norm.weight', 'layer_norm.bias', 'decoder.weight', 'decoder.bias', 'bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing RobertaForMaskedLM\n",
    "\n",
    "my_mlm = my_roberta.RobertaForMaskedLM.init(Vocab, my_config, key=key)\n",
    "state_mlm = my_mlm.to_state_dict()\n",
    "\n",
    "state_mlm = {k: torch.from_numpy(np.array(v)) for k, v in state_mlm.items()}\n",
    "\n",
    "# if \"lm_head.decoder.bias\" in state:\n",
    "#     print(state[\"lm_head.decoder.bias\"])\n",
    "# else:\n",
    "#     print(f\"RobertaForMaskedLM, {state.keys()}\")\n",
    "\n",
    "state_mlm[\"lm_head.bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "\n",
    "print(state_mlm.keys())\n",
    "\n",
    "hf_mlm = hf_roberta.RobertaForMaskedLM(hf_config)\n",
    "hf_mlm.load_state_dict(state_mlm, strict=True)\n",
    "\n",
    "# Testing RobertaModel\n",
    "\n",
    "key_rob, key_head = jrandom.split(key, 2)\n",
    "\n",
    "my_model = my_roberta.RobertaModel.init(Vocab, my_config, add_pooling_layer=False, key=key_rob)\n",
    "state_model = my_model.to_state_dict()\n",
    "\n",
    "state_model = {k: torch.from_numpy(np.array(v)) for k, v in state_model.items()}\n",
    "\n",
    "print(state_model.keys())\n",
    "\n",
    "hf_model = hf_roberta.RobertaModel(hf_config, add_pooling_layer=False)\n",
    "hf_model.load_state_dict(state_model, strict=True)\n",
    "\n",
    "# Testing RobertaLMHead\n",
    "\n",
    "my_head = my_roberta.RobertaLMHead.init(Vocab, my_config, key=key_head)\n",
    "state_head = my_head.to_state_dict()\n",
    "\n",
    "state_head = {k: torch.from_numpy(np.array(v)) for k, v in state_head.items()}\n",
    "\n",
    "state_head[\"bias\"] = torch.zeros(hf_config.vocab_size)\n",
    "\n",
    "print(state_head.keys())\n",
    "\n",
    "hf_head = hf_roberta.RobertaLMHead(hf_config)\n",
    "hf_head.load_state_dict(state_head, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_rob, k_lm = jrandom.split(key, 2)\n",
    "\n",
    "# MLM\n",
    "my_output_mlm = my_mlm(input_embeds = input_embeds, attention_mask=mask, key=key)\n",
    "hf_output_mlm = hf_mlm(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "# Model + LM\n",
    "my_output_model = my_model(input_embeds = input_embeds, attention_mask=mask, key=k_rob)\n",
    "my_output = my_head(my_output_model[0], key=k_lm)\n",
    "\n",
    "hf_output_model = hf_model(inputs_embeds = input_embeds_torch, attention_mask=mask_torch, return_dict=False)\n",
    "hf_output = hf_head(hf_output_model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaModel: acc: 1.0 \t norms: (tensor(888.5305), tensor(888.5305)) \t diffs: 3.802756225468329e-07\n",
      "Roberta Model + LM head: acc: 1.0 \t norms: (tensor(7087.6094), tensor(7087.6094)) \t diffs: 5.642933729177457e-07\n",
      "MLM: acc: 1.0 \t norms: (tensor(7087.6094), tensor(7087.6094)) \t diffs: 5.642933729177457e-07\n",
      "my RobertaModel + LM head vs MLM: acc: 1.0 \t norms: (tensor(7087.6094), tensor(7087.6094)) \t diffs: 0.0\n",
      "hf RobertaModel + LM head vs MLM: acc: 1.0 \t norms: (tensor(7087.6094), tensor(7087.6094)) \t diffs: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"RobertaModel: {check(my_output_model[0].array, hf_output_model[0].detach())[3]}\")\n",
    "print(f\"Roberta Model + LM head: {check(my_output.array, hf_output.detach())[3]}\")\n",
    "print(f\"MLM: {check(my_output_mlm[0].array, hf_output_mlm[0].detach())[3]}\")\n",
    "\n",
    "print(f\"my RobertaModel + LM head vs MLM: {check(my_output.array, my_output_mlm[0].array)[3]}\")\n",
    "print(f\"hf RobertaModel + LM head vs MLM: {check(hf_output.detach(), hf_output_mlm[0].detach())[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM\n",
    "\n",
    "my_output_mlm = my_mlm(input_ids = input_ids, attention_mask=mask, key=key)\n",
    "hf_output_mlm = hf_mlm(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "\n",
    "# Model + LM\n",
    "\n",
    "my_output_model = my_model(input_ids = input_ids, attention_mask=mask, key=k_rob)\n",
    "my_output = my_head(my_output_model[0], key=k_lm)\n",
    "\n",
    "hf_output_model = hf_model(input_ids = input_ids_torch, attention_mask=mask_torch, return_dict=False)\n",
    "hf_output = hf_head(hf_output_model[0])\n",
    "\n",
    "\n",
    "# Checks\n",
    "\n",
    "# Notes\n",
    "# embeds works between hf and my, and within model->head and mlm \n",
    "# ids does not work between hf and my for mlm or within hf for model->head and mlm - so hf mlm is doing something weird.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaModel: acc: 1.0 \t norms: (tensor(888.5287), tensor(888.5287)) \t diffs: 3.736330143055966e-07\n",
      "Roberta Model + LM head: acc: 1.0 \t norms: (tensor(7065.4971), tensor(7065.4971)) \t diffs: 5.507896503331722e-07\n",
      "MLM: acc: 0.0014420458728273227 \t norms: (tensor(7065.4971), tensor(7062.7324)) \t diffs: 0.07865540683269501\n",
      "my RobertaModel + LM head vs MLM: acc: 1.0 \t norms: (tensor(7065.4971), tensor(7065.4971)) \t diffs: 0.0\n",
      "hf RobertaModel + LM head vs MLM: acc: 0.0014420071674599332 \t norms: (tensor(7065.4971), tensor(7062.7324)) \t diffs: 0.07865539938211441\n"
     ]
    }
   ],
   "source": [
    "print(f\"RobertaModel: {check(my_output_model[0].array, hf_output_model[0].detach())[3]}\")\n",
    "print(f\"Roberta Model + LM head: {check(my_output.array, hf_output.detach())[3]}\")\n",
    "print(f\"MLM: {check(my_output_mlm[0].array, hf_output_mlm[0].detach())[3]}\")\n",
    "\n",
    "print(f\"my RobertaModel + LM head vs MLM: {check(my_output.array, my_output_mlm[0].array)[3]}\")\n",
    "print(f\"hf RobertaModel + LM head vs MLM: {check(hf_output.detach(), hf_output_mlm[0].detach())[3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
