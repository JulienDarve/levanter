checkpoint_path: gs://marin-ckpt-eu-w4/marin/olmoish7b_v5e512_0612/dlwh_7b0612/step-200000/
output_dir: gs://marin-ckpt-eu-w4/marin/olmoish7b_v5e512_0612/hf_model/
upload_to_hf: stanford-crfm/llama-7b-dolma
model:  # 7B class model
  type: llama
  seq_len: 2048
  hidden_dim: 4096
  intermediate_dim: 11008
  num_layers: 32
  num_heads: 32
  num_kv_heads: 32
  use_flash_attention: True
#  flash_attention_block_size: 1024
  use_bias: false
  use_layer_norm_weight: false
tokenizer: "allenai/OLMo-1B"
save_tokenizer: true
