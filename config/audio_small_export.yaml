model:
  type: gpt2
  hidden_dim: 768
  num_heads: 12
  num_layers: 12
  seq_len: 8192
  scale_attn_by_inverse_layer_idx: true
  gradient_checkpointing: true
  use_flash_attention: true
  flash_attention_block_size: 2048